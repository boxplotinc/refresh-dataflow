{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d62938b-b48b-4c64-b7cd-6e7e656a9f26",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### **This notebook will implement a basic framework for an incremental update in a dataflow where the standard incremental update does not work because of limitations in the datasource with query folding and bucket size limits.**\n",
    "\n",
    "The way this works is as below:\n",
    "1. This notebook activity is configured in a pipeline with the following base parameters of the notebook \n",
    "- **dataflow_id** = id of the dataflow\n",
    "- **workspace_id** = id of the workspace\n",
    "- **dataflow_name** = name of the dataflow\n",
    "- **initial_load_from_date** = historical data requirements for this dataflow. The first load will extract data starting from this date.\n",
    "- **bucket_size_in_days** = bucket size for each load in days. The first load will load the history in these bucket sizes to limit the data.\n",
    "- **reinitialize_dataflow** = refresh the entire data and ignore any previous loads.\n",
    "- **incrementally_update_last_n_days** = incrementally update the last n days every day, default is 1\n",
    "- **destination_table** = destination table in the warehouse where the data is written\n",
    "- **incremental_update_column** = column in the destination table that is used for the incremental update\n",
    "\n",
    "2. Following parameters should be configured inside this notebook which are defined as constants:\n",
    "    - SCHEMA = \"[xxx].[dbo]\" - This is the schema of the warehouse where the incremental update table resides\n",
    "    - INCREMENTAL_TABLE = \"[Incremental Update]\" - Name of the incremental update table\n",
    "    - CONNECTION_ARTIFACT = \"xxx\" - Name of the artifact for e.g. the warehouse name\n",
    "    - CONNECTION_ARTIFACT_ID = \"xxx\" - Technical ID of the artifact for e.g. the warehouse id\n",
    "    - CONNECTION_ARTIFACT_TYPE = \"xxx\" - Type of articfact for e.g. Warehouse\n",
    "\n",
    "3. Based on the configured parameters, this notebook reads the Incremental Update table in the warehouse for the dataflow_id to determine if there was a previous refresh. If there was no previous refresh it assumes that this is an initial refresh and proceeds with loading the history. If there was a previous refresh then it reads the last range_start and range_end datetime parameters used for the refresh and the status of the last refresh. If the incremental update meta data table does not exist on the database it creates this table and inserts the entry for the dataflow id.\n",
    "\n",
    "4. If this is the initial load, then the notebook calculates the first load range_start and range_end parameters based on the initial_load_from_date and bucket_size_in_days, and updates these values in the Incremental Update table in the warehouse, and sets the status of the load as Running. It repeats the process till all the history is refreshed till yesterday.\n",
    "\n",
    "5. If this is the next incremental load, the notebook calculates the range_start and range_end based on the status of the last refresh, the range_start and range_end of the last load, the incrementally_update_last_n_days and bucket_size_in_days parameters.\n",
    "\n",
    "6. The dataflow activity will read these range_start and range_end parameters from the Incremental Update table in the warehouse and refresh then execute the power query to refresh the data accordingly to the destination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14028713-9697-4566-824a-aef63c4d6f55",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters passed from the pipeline as base parameters of the notebook activity\n",
    "# Workspace id\n",
    "workspace_id = ''\n",
    "\n",
    "# Dataflow id\n",
    "dataflow_id = ''\n",
    "\n",
    "# Dataflow name\n",
    "dataflow_name = ''\n",
    "\n",
    "# Initial load from date for the first load\n",
    "initial_load_from_date = ''\n",
    "\n",
    "# Bucket size for each load\n",
    "bucket_size_in_days = 1\n",
    "\n",
    "# Reinitialize dataflow\n",
    "reinitialize_dataflow = False\n",
    "\n",
    "# Incrementally update last n days\n",
    "incrementally_update_last_n_days = 1\n",
    "\n",
    "# Destination table\n",
    "destination_table = ''\n",
    "\n",
    "# Incremental update column\n",
    "incremental_update_column = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b53d9c-fc4e-4f85-b08f-f00e5fb8c6a9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import sempy.fabric as fabric\n",
    "from typing import Optional, Dict, Any, Union, Tuple, List\n",
    "\n",
    "# Constants\n",
    "SCHEMA = \"[<your Warehouse or Lakehouse name>].[dbo]\"\"\n",
    "INCREMENTAL_TABLE = \"[Incremental Update]\"\n",
    "CONNECTION_ARTIFACT = \"<your Warehouse or Lakehouse name>\"\n",
    "CONNECTION_ARTIFACT_ID = \"<your Warehouse or Lakehouse id>\"\n",
    "CONNECTION_ARTIFACT_TYPE = \"<Warehouse or Lakehouse>\"\n",
    "\n",
    "class DataflowRefresher:\n",
    "    \"\"\"\n",
    "    Class to incrementally refresh a dataflow in Microsoft Fabric\n",
    "    with support for initial loads and incremental updates\n",
    "    \"\"\"\n",
    "    def __init__(self, client, artifact: str, artifact_id: str, artifact_type: str,\n",
    "                 schema: str, incremental_table: str, log_level: int):\n",
    "        \"\"\"\n",
    "        Initialize the DataflowRefresher\n",
    "        \n",
    "        Args:\n",
    "            client: API client for Microsoft Fabric\n",
    "            artifact: Name of the artifact to connect to (e.g., \"The Beer Store\")\n",
    "            artifact_id: ID of the artifact (e.g., workspace ID)\n",
    "            artifact_type: Type of the artifact (e.g., \"Warehouse\")\n",
    "            schema: Database schema name including brackets, e.g. \"[The Beer Store].[dbo]\"\n",
    "            incremental_table: Name of the table tracking incremental updates (without schema)\n",
    "            log_level: level of logging\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.artifact = artifact\n",
    "        self.artifact_id = artifact_id\n",
    "        self.artifact_type = artifact_type\n",
    "        self.connection = None\n",
    "        self.schema = schema\n",
    "        self.incremental_table = f\"{schema}.{incremental_table}\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(log_level)\n",
    "        self._ensure_connection()\n",
    "        self.create_incremental()\n",
    "    \n",
    "    def _create_connection(self):\n",
    "        \"\"\"\n",
    "        Create a new connection to the artifact\n",
    "        \n",
    "        Returns:\n",
    "            A new database connection\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import notebookutils.data\n",
    "            return notebookutils.data.connect_to_artifact(\n",
    "                self.artifact, self.artifact_id, self.artifact_type)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating connection: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _ensure_connection(self):\n",
    "        \"\"\"\n",
    "        Ensure that we have a valid database connection, creating a new one if needed\n",
    "        \n",
    "        Returns:\n",
    "            A valid database connection\n",
    "        \"\"\"\n",
    "        if self.connection is None:\n",
    "            self.connection = self._create_connection()\n",
    "            return self.connection\n",
    "            \n",
    "        # Test if the existing connection is still valid\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(\"SELECT 1\")\n",
    "            cursor.fetchall()\n",
    "            cursor.close()\n",
    "            return self.connection\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Connection test failed, reconnecting: {e}\")\n",
    "            self._close_connection()\n",
    "            self.connection = self._create_connection()\n",
    "            return self.connection\n",
    "    \n",
    "    def _close_connection(self):\n",
    "        \"\"\"Close the current connection if it exists\"\"\"\n",
    "        if self.connection is not None:\n",
    "            try:\n",
    "                self.connection.close()\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error closing connection: {e}\")\n",
    "            finally:\n",
    "                self.connection = None\n",
    "    \n",
    "    def _execute_with_retry(self, sql, params=None, commit=True, max_retries=3):\n",
    "        \"\"\"\n",
    "        Execute a SQL statement with automatic retry on connection issues\n",
    "        \n",
    "        Args:\n",
    "            sql: SQL statement to execute\n",
    "            params: Parameters for the SQL statement\n",
    "            commit: Whether to commit the transaction\n",
    "            max_retries: Maximum number of retries\n",
    "            \n",
    "        Returns:\n",
    "            Database cursor\n",
    "        \"\"\"\n",
    "        retries = 0\n",
    "        last_error = None\n",
    "        \n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                # Ensure we have a valid connection\n",
    "                self._ensure_connection()\n",
    "                \n",
    "                # Execute the SQL\n",
    "                cursor = self.connection.execute(sql, params or ())\n",
    "                \n",
    "                # Commit if requested\n",
    "                if commit:\n",
    "                    self.connection.commit()\n",
    "                    \n",
    "                return cursor\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                self.logger.warning(f\"Database operation failed (attempt {retries+1}/{max_retries}): {e}\")\n",
    "                self._close_connection()  # Force reconnection on next attempt\n",
    "                retries += 1\n",
    "                \n",
    "                # Small delay before retry\n",
    "                if retries < max_retries:\n",
    "                    time.sleep(1)\n",
    "        \n",
    "        # If we get here, we've exhausted retries\n",
    "        self.logger.error(f\"Database operation failed after {max_retries} attempts: {last_error}\")\n",
    "        raise last_error\n",
    "    \n",
    "    def create_incremental(self) -> bool:\n",
    "        \"\"\"\n",
    "        Create the incremental update meta data table if it does not exist\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sql = f\"\"\"\n",
    "                IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'Incremental Update' AND schema_id = SCHEMA_ID('dbo'))\n",
    "                BEGIN\n",
    "                CREATE TABLE {self.incremental_table}\n",
    "                (\n",
    "                    [dataflow_id] [varchar](60) NOT NULL,\n",
    "                    [workspace_id] [VARCHAR](60) NOT NULL,\n",
    "                    [dataflow_name] [varchar](60) NULL,\n",
    "                    [initial_load_from_date] [datetime2](3) NOT NULL,\n",
    "                    [bucket_size_in_days] [int] NOT NULL,\n",
    "                    [incrementally_update_last_n_days] [int] NOT NULL,\n",
    "                    [destination_table] [VARCHAR](60) NOT NULL,\n",
    "                    [incremental_update_column] [VARCHAR](60) NOT NULL,\n",
    "                    [update_time] [datetime2](3) NULL,\n",
    "                    [status] [varchar](50) NOT NULL,\n",
    "                    [range_start] [datetime2](3) NULL,\n",
    "                    [range_end] [datetime2](3) NULL\n",
    "                )\n",
    "                END\n",
    "            \"\"\"\n",
    "            self._execute_with_retry(sql)\n",
    "            self.logger.info(f\"Successfully created incremental update table in the database in case it did not exist\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting incremental refresh data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_incremental(self, dataflow_id: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the last refresh details from the tracking table\n",
    "        \n",
    "        Args:\n",
    "            dataflow_id: ID of the dataflow\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with the last refresh details or None if no records found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sql = f\"\"\"\n",
    "                SELECT TOP (1) [dataflow_id],\n",
    "                         [update_time],\n",
    "                         [status],\n",
    "                         [range_start],\n",
    "                         [range_end]\n",
    "                FROM {self.incremental_table}\n",
    "                WHERE [dataflow_id] = ?\n",
    "                ORDER BY [update_time] DESC\n",
    "            \"\"\"\n",
    "            cursor = self._execute_with_retry(sql, (dataflow_id,), commit=False)\n",
    "            columns = [column[0] for column in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            data = [tuple(row) for row in data]\n",
    "            \n",
    "            if not data:\n",
    "                self.logger.info(f\"No previous refresh records found for dataflow {dataflow_id}\")\n",
    "                return None\n",
    "                \n",
    "            return pd.DataFrame(data, columns=columns)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting incremental refresh data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def insert_into_incremental(self, dataflow_id: str, workspace_id: str, dataflow_name: str, initial_load_from_date: str,\n",
    "                            bucket_size_in_days: int, incrementally_update_last_n_days: int, destination_table: str,\n",
    "                            incremental_update_column: str, status: str, \n",
    "                            range_start: datetime, range_end: datetime) -> bool:\n",
    "        \"\"\"\n",
    "        Insert into incremental table in the warehouse\n",
    "        \n",
    "        Args:\n",
    "            dataflow_id: ID of the dataflow\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_name: Name or description of the dataflow\n",
    "            initial_load_from_date: Initial load from date\n",
    "            bucket_size_in_days: Bucket size of each refresh in days\n",
    "            incrementally_update_last_n_days: Incremental refresh bucket size\n",
    "            destination_table: Destination table in the warehouse where data is written\n",
    "            incremental_update_column: Column of the destination table used for incremental update\n",
    "            status: Status of the refresh operation\n",
    "            range_start: Start of the refresh date range\n",
    "            range_end: End of the refresh date range\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            start_formatted = range_start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            end_formatted = range_end.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            \n",
    "            sql = f\"\"\"\n",
    "                INSERT INTO {self.incremental_table}\n",
    "                (\n",
    "                    [dataflow_id], \n",
    "                    [workspace_id],\n",
    "                    [dataflow_name],\n",
    "                    [initial_load_from_date],\n",
    "                    [bucket_size_in_days],\n",
    "                    [incrementally_update_last_n_days],\n",
    "                    [destination_table],\n",
    "                    [incremental_update_column],\n",
    "                    [update_time],\n",
    "                    [status],\n",
    "                    [range_start],\n",
    "                    [range_end]\n",
    "                )\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            params = (dataflow_id, workspace_id, dataflow_name, initial_load_from_date, \n",
    "                     bucket_size_in_days, incrementally_update_last_n_days, destination_table, \n",
    "                     incremental_update_column, current_time, status, start_formatted, end_formatted)\n",
    "            \n",
    "            self._execute_with_retry(sql, params)\n",
    "            self.logger.info(f\"Successfully inserted record for dataflow {dataflow_id}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error inserting into incremental table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_incremental(self, dataflow_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete the entry from the tracking table\n",
    "        \n",
    "        Args:\n",
    "            dataflow_id: ID of the dataflow\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sql = f\"\"\"\n",
    "                DELETE\n",
    "                FROM {self.incremental_table}\n",
    "                WHERE [dataflow_id] = ?\n",
    "            \"\"\"\n",
    "            self._execute_with_retry(sql, (dataflow_id,))\n",
    "            self.logger.info(f\"Successfully deleted old record for dataflow {dataflow_id}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Could not delete old record for dataflow: {e}\")\n",
    "            raise\n",
    "\n",
    "    def update_incremental(self, dataflow_id: str, status: str, \n",
    "                        range_start: datetime, range_end: datetime) -> bool:\n",
    "        \"\"\"\n",
    "        Update incremental table in the warehouse with the new parameters\n",
    "        \n",
    "        Args:\n",
    "            dataflow_id: ID of the dataflow\n",
    "            status: Status of the refresh operation\n",
    "            range_start: Start of the refresh date range\n",
    "            range_end: End of the refresh date range\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            start_formatted = range_start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            end_formatted = range_end.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            \n",
    "            sql = f\"\"\"\n",
    "                UPDATE {self.incremental_table}\n",
    "                SET \n",
    "                [status] = ?,\n",
    "                [update_time] = ?,\n",
    "                [range_start] = ?,\n",
    "                [range_end] = ?\n",
    "                WHERE [dataflow_id] = ?\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor = self._execute_with_retry(sql, (status, current_time, \n",
    "                                               start_formatted, end_formatted, dataflow_id))\n",
    "            rows_affected = cursor.rowcount\n",
    "            \n",
    "            if rows_affected > 0:\n",
    "                self.logger.info(f\"Successfully updated record for dataflow {dataflow_id}\")\n",
    "                return True\n",
    "            else:\n",
    "                self.logger.warning(f\"No records updated for dataflow {dataflow_id}, record may not exist\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error updating incremental table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_data(self, table: str, column: str, \n",
    "                 range_start: Union[datetime, str], \n",
    "                 range_end: Union[datetime, str],\n",
    "                 dataflow_id: Optional[str] = None) -> int:\n",
    "        \"\"\"\n",
    "        Delete any overlapping data from the previous refreshes\n",
    "        \n",
    "        Args:\n",
    "            table: Target table name\n",
    "            column: Date column to use for filtering\n",
    "            range_start: Start of the date range to delete (datetime or string)\n",
    "            range_end: End of the date range to delete (datetime or string)\n",
    "            dataflow_id: Optional ID of the dataflow for logging purposes\n",
    "            \n",
    "        Returns:\n",
    "            Number of rows deleted\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Format dates if they're datetime objects\n",
    "            start_formatted = range_start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] if isinstance(range_start, datetime) else range_start\n",
    "            end_formatted = range_end.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] if isinstance(range_end, datetime) else range_end\n",
    "            \n",
    "            sql = f\"\"\"\n",
    "                DELETE FROM {self.schema}.[{table}]\n",
    "                WHERE [{column}] BETWEEN ? AND ?\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor = self._execute_with_retry(sql, (start_formatted, end_formatted))\n",
    "            rows_deleted = cursor.rowcount\n",
    "            \n",
    "            log_id = f\" for {dataflow_id}\" if dataflow_id else \"\"\n",
    "            self.logger.info(f\"Successfully deleted {rows_deleted} overlapping records{log_id} in table {table} between {start_formatted} and {end_formatted}\")\n",
    "            \n",
    "            return rows_deleted\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error deleting overlapping records: {e}\")\n",
    "            raise\n",
    "\n",
    "    def refresh_dataflow(self, workspace_id: str, dataflow_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Refresh dataflow and return the response\n",
    "        \n",
    "        Args:\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_id: ID of the dataflow\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the response from the refresh API\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.post(f\"/v1.0/myorg/groups/{workspace_id}/dataflows/{dataflow_id}/refreshes\", json={\"refreshRequest\": \"y\"})\n",
    "            self.logger.info(f\"Successfully triggered refresh for dataflow {dataflow_id}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error triggering refresh: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_latest_refresh_status(self, workspace_id: str, dataflow_id: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get refresh status of a dataflow\n",
    "        \n",
    "        Args:\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_id: ID of the dataflow\n",
    "            \n",
    "        Returns:\n",
    "            Status of the latest refresh operation or None if not available\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.get(f\"/v1.0/myorg/groups/{workspace_id}/dataflows/{dataflow_id}/transactions\")\n",
    "            transactions = response.json()['value']\n",
    "            if transactions and len(transactions) > 0:\n",
    "                latest_transaction = transactions[0]\n",
    "                status = latest_transaction.get(\"status\", \"Unknown\")\n",
    "                self.logger.info(f\"Latest refresh status for dataflow {dataflow_id}: {status}\")\n",
    "                return status\n",
    "            else:\n",
    "                self.logger.warning(f\"No refresh transactions found for dataflow {dataflow_id}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking refresh status: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def wait_for_refresh_completion(self, workspace_id: str, dataflow_id: str, \n",
    "                                 timeout_minutes: int = 60, \n",
    "                                 check_interval_seconds: int = 30) -> str:\n",
    "        \"\"\"\n",
    "        Wait for a dataflow refresh to complete\n",
    "        \n",
    "        Args:\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_id: ID of the dataflow\n",
    "            timeout_minutes: Maximum wait time in minutes\n",
    "            check_interval_seconds: Interval between status checks in seconds\n",
    "            \n",
    "        Returns:\n",
    "            Final status of the refresh operation\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Waiting for dataflow {dataflow_id} refresh to complete (timeout: {timeout_minutes} minutes)\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        timeout = timedelta(minutes=timeout_minutes)\n",
    "        \n",
    "        while datetime.now() - start_time < timeout:\n",
    "            # Close existing connection before potentially long wait\n",
    "            # This is critical to avoid idle timeout issues\n",
    "            self._close_connection()\n",
    "            \n",
    "            status = self.get_latest_refresh_status(workspace_id, dataflow_id)\n",
    "            \n",
    "            if status in [\"Success\", \"Failed\", \"Cancelled\"]:\n",
    "                self.logger.info(f\"Dataflow refresh completed with status: {status}\")\n",
    "                return status\n",
    "                \n",
    "            self.logger.info(f\"Current status: {status}, checking again in {check_interval_seconds} seconds\")\n",
    "            time.sleep(check_interval_seconds)\n",
    "            \n",
    "        self.logger.warning(f\"Refresh timeout reached after {timeout_minutes} minutes\")\n",
    "        return \"Timeout\"\n",
    "\n",
    "    def _parse_date(self, date_str: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Parse a date string into a datetime object\n",
    "        \n",
    "        Args:\n",
    "            date_str: Date string in various formats\n",
    "            \n",
    "        Returns:\n",
    "            Datetime object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try different formats\n",
    "            for fmt in ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S']:\n",
    "                try:\n",
    "                    return datetime.strptime(date_str, fmt)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # If all formats fail, raise exception\n",
    "            raise ValueError(f\"Unable to parse date string: {date_str}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing date: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _get_date_ranges(self, start_date: datetime, end_date: datetime, bucket_size_days: int) -> List[Tuple[datetime, datetime]]:\n",
    "        \"\"\"\n",
    "        Split a date range into smaller buckets\n",
    "        \n",
    "        Args:\n",
    "            start_date: Start date of the range\n",
    "            end_date: End date of the range\n",
    "            bucket_size_days: Size of each bucket in days\n",
    "            \n",
    "        Returns:\n",
    "            List of (start_date, end_date) tuples for each bucket\n",
    "        \"\"\"\n",
    "        date_ranges = []\n",
    "        current_start = start_date\n",
    "        \n",
    "        while current_start < end_date:\n",
    "            current_end = min(current_start + timedelta(days=bucket_size_days), end_date)\n",
    "            \n",
    "            # Set time to 23:59:59 for the end date of each bucket except the last one\n",
    "            if current_end < end_date:\n",
    "                current_end = datetime(current_end.year, current_end.month, current_end.day, 23, 59, 59)\n",
    "                \n",
    "            date_ranges.append((current_start, current_end))\n",
    "            \n",
    "            # Start the next bucket from the day after the current end\n",
    "            current_start = current_end + timedelta(seconds=1)\n",
    "        \n",
    "        return date_ranges\n",
    "\n",
    "    def execute_incremental_refresh(self, \n",
    "                               workspace_id: str, \n",
    "                               dataflow_id: str,\n",
    "                               dataflow_name: str,\n",
    "                               destination_table: str,\n",
    "                               incremental_update_column: str,\n",
    "                               initial_load_from_date: str = None,\n",
    "                               bucket_size_in_days: int = 30,\n",
    "                               reinitialize_dataflow: bool = False,\n",
    "                               incrementally_update_last_n_days: int = None,\n",
    "                               wait_for_completion: bool = True,\n",
    "                               timeout_minutes: int = 120) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute a complete incremental refresh workflow following the specified logic\n",
    "        \n",
    "        Args:\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_id: ID of the dataflow\n",
    "            destination_table: Name of the destination table\n",
    "            incremental_update_column: Column used for incremental updates\n",
    "            initial_load_from_date: Start date for the initial load (required for first load)\n",
    "            bucket_size_in_days: Size of each refresh bucket in days\n",
    "            reinitialize_dataflow: Whether to reinitialize the dataflow\n",
    "            incrementally_update_last_n_days: Number of days to update incrementally\n",
    "            wait_for_completion: Whether to wait for refresh completion\n",
    "            timeout_minutes: Timeout when waiting for completion\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing execution results and statistics\n",
    "        \"\"\"\n",
    "        # Ensure we have a fresh connection at the start\n",
    "        self._ensure_connection()\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        self.logger.info(f\"Starting incremental refresh for dataflow {dataflow_id}\")\n",
    "        \n",
    "        # Get yesterday's date at 23:59:59 as the default end date\n",
    "        yesterday = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\n",
    "        yesterday_end = datetime(yesterday.year, yesterday.month, yesterday.day, 23, 59, 59)\n",
    "        \n",
    "        # Initialize statistics\n",
    "        stats = {\n",
    "            \"dataflow_id\": dataflow_id,\n",
    "            \"start_time\": start_time,\n",
    "            \"buckets_refreshed\": 0,\n",
    "            \"successful_refreshes\": 0,\n",
    "            \"failed_refreshes\": 0,\n",
    "            \"current_status\": \"Started\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check if we need to reinitialize the dataflow\n",
    "            if reinitialize_dataflow:\n",
    "                self.logger.info(f\"Reinitializing dataflow {dataflow_id} as requested\")\n",
    "                self.delete_incremental(dataflow_id)\n",
    "                last_refresh_data = None\n",
    "            else:\n",
    "                # Get the last refresh record\n",
    "                last_refresh_df = self.get_incremental(dataflow_id)\n",
    "                last_refresh_data = last_refresh_df.iloc[0].to_dict() if last_refresh_df is not None else None\n",
    "            \n",
    "            # Case 1: No previous refresh or reinitializing dataflow\n",
    "            if last_refresh_data is None:\n",
    "                self.logger.info(f\"No previous refresh found or reinitializing for dataflow {dataflow_id}\")\n",
    "                \n",
    "                if not initial_load_from_date:\n",
    "                    raise ValueError(\"initial_load_from_date is required for the first load\")\n",
    "                \n",
    "                start_date = self._parse_date(initial_load_from_date)\n",
    "                end_date = yesterday_end\n",
    "                \n",
    "                self.logger.info(f\"Performing initial load from {start_date} to {end_date}\")\n",
    "                \n",
    "                # Split the date range into buckets\n",
    "                date_ranges = self._get_date_ranges(start_date, end_date, bucket_size_in_days)\n",
    "                \n",
    "                stats[\"date_ranges\"] = len(date_ranges)\n",
    "                stats[\"start_date\"] = start_date\n",
    "                stats[\"end_date\"] = end_date\n",
    "                \n",
    "                for i, (range_start, range_end) in enumerate(date_ranges):\n",
    "                    self.logger.info(f\"Processing bucket {i+1}/{len(date_ranges)}: {range_start} to {range_end}\")\n",
    "                    \n",
    "                    # Delete data in the range\n",
    "                    self.delete_data(destination_table, incremental_update_column, range_start, range_end, dataflow_id)\n",
    "                    \n",
    "                    # Insert a record with \"Running\" status\n",
    "                    if i == 0:\n",
    "                        self.insert_into_incremental(dataflow_id, workspace_id, dataflow_name, initial_load_from_date,\n",
    "                                        bucket_size_in_days, incrementally_update_last_n_days, destination_table,\n",
    "                                        incremental_update_column, \"Running\", range_start, range_end)\n",
    "                    else:\n",
    "                        self.update_incremental(dataflow_id, \"Running\", range_start, range_end)\n",
    "                        \n",
    "                    # Close connection before long-running dataflow operation\n",
    "                    self._close_connection()\n",
    "\n",
    "                    # Trigger dataflow refresh\n",
    "                    self.refresh_dataflow(workspace_id, dataflow_id)\n",
    "                    \n",
    "                    # Wait for completion if requested\n",
    "                    if wait_for_completion:\n",
    "                        # Already closed connection before this step\n",
    "                        # Add a small buffer of 5 seconds just to make sure the status is updated\n",
    "                        time.sleep(5)\n",
    "                        status = self.wait_for_refresh_completion(workspace_id, dataflow_id, timeout_minutes)\n",
    "                        \n",
    "                        # Ensure fresh connection for database operations\n",
    "                        self._ensure_connection()\n",
    "                        \n",
    "                        # Update status in the incremental table\n",
    "                        self.update_incremental(dataflow_id, status, range_start, range_end)\n",
    "\n",
    "                        stats[\"buckets_refreshed\"] += 1\n",
    "                        if status == \"Success\":\n",
    "                            stats[\"successful_refreshes\"] += 1\n",
    "                        else:\n",
    "                            stats[\"failed_refreshes\"] += 1\n",
    "                            self.logger.warning(f\"Refresh failed for range {range_start} to {range_end} with status {status}\")\n",
    "                    else:\n",
    "                        self.logger.info(\"Not waiting for completion, continuing with next bucket\")\n",
    "                        stats[\"buckets_refreshed\"] += 1\n",
    "            \n",
    "            # Case 2: Previous refresh exists and we're not reinitializing\n",
    "            else:\n",
    "                last_status = last_refresh_data.get('status')\n",
    "                last_range_start = last_refresh_data.get('range_start')\n",
    "                last_range_end = last_refresh_data.get('range_end')\n",
    "                \n",
    "                self.logger.info(f\"Previous refresh found with status '{last_status}' for range {last_range_start} to {last_range_end}\")\n",
    "                \n",
    "                # Case 2a: Previous refresh was not successful, retry it\n",
    "                if last_status not in [\"Completed\", \"Success\", \"Successful\"]:\n",
    "                    self.logger.info(f\"Previous refresh was not successful, retrying for range {last_range_start} to {last_range_end}\")\n",
    "                    \n",
    "                    # Parse dates\n",
    "                    range_start = last_range_start if isinstance(last_range_start, datetime) else self._parse_date(last_range_start)\n",
    "                    range_end = last_range_end if isinstance(last_range_end, datetime) else self._parse_date(last_range_end)\n",
    "                    \n",
    "                    # Delete data in the range\n",
    "                    self.delete_data(destination_table, incremental_update_column, range_start, range_end, dataflow_id)\n",
    "                    \n",
    "                    # Update status to \"Running\"\n",
    "                    self.update_incremental(dataflow_id, \"Running\", range_start, range_end)\n",
    "                    \n",
    "                    # Close connection before long-running operation\n",
    "                    self._close_connection()\n",
    "                    \n",
    "                    # Trigger dataflow refresh\n",
    "                    self.refresh_dataflow(workspace_id, dataflow_id)\n",
    "                    \n",
    "                    # Wait for completion if requested\n",
    "                    if wait_for_completion:\n",
    "                        # Already closed connection above\n",
    "                        # Add a small buffer of 5 seconds before checking status\n",
    "                        time.sleep(5)\n",
    "                        status = self.wait_for_refresh_completion(workspace_id, dataflow_id, timeout_minutes)\n",
    "                        \n",
    "                        # Ensure fresh connection for database operations\n",
    "                        self._ensure_connection()\n",
    "                        \n",
    "                        self.update_incremental(dataflow_id, status, range_start, range_end)\n",
    "                        \n",
    "                        stats[\"buckets_refreshed\"] += 1\n",
    "                        if status == \"Success\":\n",
    "                            stats[\"successful_refreshes\"] += 1\n",
    "                        else:\n",
    "                            stats[\"failed_refreshes\"] += 1\n",
    "                            self.logger.warning(f\"Retry refresh failed for range {range_start} to {range_end} with status {status}\")\n",
    "                    else:\n",
    "                        stats[\"buckets_refreshed\"] += 1\n",
    "                \n",
    "                # Case 2b: Previous refresh was successful, continue with next incremental update\n",
    "                else:\n",
    "                    self.logger.info(\"Previous refresh was successful, calculating next incremental update range\")\n",
    "                    \n",
    "                    # Parse the last range end date\n",
    "                    last_end_date = last_range_end if isinstance(last_range_end, datetime) else self._parse_date(last_range_end)\n",
    "\n",
    "                    # End date is yesterday\n",
    "                    range_end = yesterday_end\n",
    "\n",
    "                    # If incrementally_update_last_n_days is provided, use it to potentially overlap with previous data\n",
    "                    if incrementally_update_last_n_days:\n",
    "                        possible_start = yesterday - timedelta(days=incrementally_update_last_n_days)\n",
    "                        # Use the minimum of the two possible start dates to ensure there are no gaps\n",
    "                        # If last_end_date + 1 second is earlier, use that to continue from where we left off\n",
    "                        # If possible_start is earlier, use that to ensure we include the last N days\n",
    "                        range_start = min(last_end_date + timedelta(seconds=1), possible_start)\n",
    "                        self.logger.info(f\"Using start date {range_start} (minimum of last_end_date+1 second and last {incrementally_update_last_n_days} days)\")\n",
    "                    else:\n",
    "                        # Start from the day after the last end date\n",
    "                        range_start = last_end_date + timedelta(seconds=1)\n",
    "                        self.logger.info(f\"Using start date {range_start} (continuing from last end date)\")\n",
    "\n",
    "                    # Only proceed if there's actual data to refresh\n",
    "                    if range_start < range_end:\n",
    "                        self.logger.info(f\"Performing incremental update from {range_start} to {range_end}\")\n",
    "\n",
    "                        # Split the date range into buckets\n",
    "                        date_ranges = self._get_date_ranges(range_start, range_end, bucket_size_in_days)\n",
    "\n",
    "                        stats[\"date_ranges\"] = len(date_ranges)\n",
    "                        stats[\"start_date\"] = range_start\n",
    "                        stats[\"end_date\"] = range_end\n",
    "                        \n",
    "                        for i, (bucket_start, bucket_end) in enumerate(date_ranges):\n",
    "                            self.logger.info(f\"Processing bucket {i+1}/{len(date_ranges)}: {bucket_start} to {bucket_end}\")\n",
    "                            \n",
    "                            # Delete data in the range\n",
    "                            self.delete_data(destination_table, incremental_update_column, bucket_start, bucket_end, dataflow_id)\n",
    "                            \n",
    "                            # Insert a record with \"Running\" status\n",
    "                            self.update_incremental(dataflow_id, \"Running\", bucket_start, bucket_end)\n",
    "                            \n",
    "                            # Close connection before long-running operation\n",
    "                            self._close_connection()\n",
    "                            \n",
    "                            # Trigger dataflow refresh\n",
    "                            self.refresh_dataflow(workspace_id, dataflow_id)\n",
    "                            \n",
    "                            # Wait for completion if requested\n",
    "                            if wait_for_completion:\n",
    "                                # Already closed connection above\n",
    "                                # Add a small buffer of 5 seconds before checking status\n",
    "                                time.sleep(5)\n",
    "                                status = self.wait_for_refresh_completion(workspace_id, dataflow_id, timeout_minutes)\n",
    "                                \n",
    "                                # Ensure fresh connection for database operations\n",
    "                                self._ensure_connection()\n",
    "                                \n",
    "                                # Update status in the incremental table\n",
    "                                self.update_incremental(dataflow_id, status, bucket_start, bucket_end)\n",
    "                                \n",
    "                                stats[\"buckets_refreshed\"] += 1\n",
    "                                if status == \"Success\":\n",
    "                                    stats[\"successful_refreshes\"] += 1\n",
    "                                else:\n",
    "                                    stats[\"failed_refreshes\"] += 1\n",
    "                                    self.logger.warning(f\"Refresh failed for range {bucket_start} to {bucket_end} with status {status}\")\n",
    "                            else:\n",
    "                                stats[\"buckets_refreshed\"] += 1\n",
    "                    else:\n",
    "                        self.logger.info(f\"No new data to refresh. Last refresh end date {last_end_date} is after or equal to start date {range_start}\")\n",
    "                        stats[\"current_status\"] = \"No new data to refresh\"\n",
    "            \n",
    "            # Calculate total duration\n",
    "            stats[\"end_time\"] = datetime.now()\n",
    "            stats[\"duration_seconds\"] = (stats[\"end_time\"] - start_time).total_seconds()\n",
    "            stats[\"current_status\"] = \"Completed\" if stats[\"failed_refreshes\"] == 0 else f\"Completed with {stats['failed_refreshes']} failures\"\n",
    "            \n",
    "            self.logger.info(f\"Incremental refresh completed for dataflow {dataflow_id}. \"\n",
    "                          f\"Total buckets: {stats['buckets_refreshed']}, \"\n",
    "                          f\"Successful: {stats['successful_refreshes']}, \"\n",
    "                          f\"Failed: {stats['failed_refreshes']}\")\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error executing incremental refresh: {str(e)}\"\n",
    "            self.logger.error(error_message)\n",
    "            \n",
    "            # Update stats with error information\n",
    "            stats[\"end_time\"] = datetime.now()\n",
    "            stats[\"duration_seconds\"] = (stats[\"end_time\"] - start_time).total_seconds()\n",
    "            stats[\"current_status\"] = f\"Failed: {str(e)}\"\n",
    "            stats[\"error\"] = str(e)\n",
    "            \n",
    "            # Ensure we close the connection on error\n",
    "            self._close_connection()\n",
    "            \n",
    "            return stats\n",
    "        finally:\n",
    "            # Make sure to close the connection when done\n",
    "            self._close_connection()\n",
    "\n",
    "# Create the power bi rest client\n",
    "client = fabric.PowerBIRestClient()\n",
    "\n",
    "# Create the dataflow refresher object\n",
    "dataflow_refresher = DataflowRefresher(\n",
    "    client, \n",
    "    CONNECTION_ARTIFACT, \n",
    "    CONNECTION_ARTIFACT_ID, \n",
    "    CONNECTION_ARTIFACT_TYPE,\n",
    "    SCHEMA, INCREMENTAL_TABLE, \n",
    "    logging.INFO)\n",
    "\n",
    "# Incrementally refresh the dataflow\n",
    "dataflow_refresher.execute_incremental_refresh(\n",
    "    workspace_id, \n",
    "    dataflow_id, \n",
    "    dataflow_name, \n",
    "    destination_table, \n",
    "    incremental_update_column,\n",
    "    initial_load_from_date, \n",
    "    bucket_size_in_days, \n",
    "    reinitialize_dataflow, \n",
    "    incrementally_update_last_n_days \n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "warehouse": {
    "default_warehouse": "5f958d46-a6bc-4186-87f8-e81095a3e6c1",
    "known_warehouses": [
     {
      "id": "5f958d46-a6bc-4186-87f8-e81095a3e6c1",
      "type": "Datawarehouse"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

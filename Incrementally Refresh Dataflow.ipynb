{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d62938b-b48b-4c64-b7cd-6e7e656a9f26",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Microsoft Fabric Dataflow Incremental Refresh Notebook\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before using this notebook, ensure you have the following:\n",
    "\n",
    "### Required Access & Permissions\n",
    "- **Microsoft Fabric Workspace**: Contributor or Admin role in the target workspace\n",
    "- **Warehouse Permissions**: Read/Write access to the warehouse where tracking tables will be created\n",
    "- **Dataflow Permissions**: Permission to trigger and monitor dataflow refreshes\n",
    "\n",
    "### Technical Requirements\n",
    "- **Microsoft Fabric Capacity**: Active Fabric capacity (F2 or higher recommended)\n",
    "- **Python Environment**: Fabric notebook environment with the following packages:\n",
    "  - `pandas` - Data manipulation\n",
    "  - `sempy.fabric` - Microsoft Fabric API client\n",
    "  - `notebookutils.data` - Fabric data connection utilities\n",
    "- **Dataflow Gen2**: Existing dataflow configured in Microsoft Fabric\n",
    "\n",
    "### Knowledge Prerequisites\n",
    "- Basic understanding of Microsoft Fabric dataflows\n",
    "- Familiarity with incremental refresh concepts\n",
    "- SQL query knowledge for troubleshooting\n",
    "- Understanding of Power Query M language for dataflow integration\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements an advanced framework for incremental data refresh in Microsoft Fabric dataflows. It is designed to handle scenarios where standard incremental refresh does not work due to limitations in the data source, such as query folding issues or bucket size constraints.\n",
    "\n",
    "The notebook supports both **regular Dataflow Gen2** and **CI/CD Dataflow Gen2** objects, automatically detecting the dataflow type and using the appropriate Microsoft Fabric REST API endpoints.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The notebook orchestrates a coordinated refresh process across multiple Fabric components:\n",
    "\n",
    "```\n",
    "┌─────────────┐\n",
    "│   Pipeline  │ (Triggers notebook with parameters)\n",
    "└──────┬──────┘\n",
    "       │\n",
    "       ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│              Notebook (DataflowRefresher)               │\n",
    "│  ┌────────────────────────────────────────────────────┐ │\n",
    "│  │ 1. Read tracking table from Warehouse              │ │\n",
    "│  │ 2. Calculate date ranges & buckets                 │ │\n",
    "│  │ 3. Update tracking table (Running status)          │ │\n",
    "│  │ 4. Call Fabric REST API to trigger dataflow        │ │\n",
    "│  │ 5. Poll for completion status                      │ │\n",
    "│  │ 6. Update tracking table (Success/Failed status)   │ │\n",
    "│  └────────────────────────────────────────────────────┘ │\n",
    "└─────────┬───────────────────────────────────┬───────────┘\n",
    "          │                                   │\n",
    "          ▼                                   ▼\n",
    "┌─────────────────────┐           ┌─────────────────────┐\n",
    "│   Warehouse         │           │  Fabric REST API    │\n",
    "│  [Incremental       │           │  - Regular DF:      │\n",
    "│   Update] Table     │           │    /dataflows/...   │\n",
    "│  - range_start      │           │  - CI/CD DF:        │\n",
    "│  - range_end        │           │    /items/.../jobs  │\n",
    "│  - status           │           └──────────┬──────────┘\n",
    "└─────────┬───────────┘                      │\n",
    "          │                                  ▼\n",
    "          │                        ┌───────────────────┐\n",
    "          └───────────────────────>│   Dataflow Gen2   │\n",
    "            (Dataflow reads range) │  (Power Query M)  │\n",
    "                                   └─────────┬─────────┘\n",
    "                                             │\n",
    "                                             ▼\n",
    "                                   ┌───────────────────┐\n",
    "                                   │  Data Source      │\n",
    "                                   │  (Filtered by     │\n",
    "                                   │   date range)     │\n",
    "                                   └─────────┬─────────┘\n",
    "                                             │\n",
    "                                             ▼\n",
    "                                   ┌───────────────────┐\n",
    "                                   │   Warehouse       │\n",
    "                                   │   Destination     │\n",
    "                                   │   Table           │\n",
    "                                   └───────────────────┘\n",
    "```\n",
    "\n",
    "**Key Flow:**\n",
    "1. **Pipeline** passes parameters to notebook\n",
    "2. **Notebook** manages the tracking table and orchestrates refresh\n",
    "3. **Tracking table** stores date ranges that the dataflow reads\n",
    "4. **Dataflow** executes with filtered date range from tracking table\n",
    "5. **Data** flows from source to warehouse destination table\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Intelligent Bucket Processing**: Splits large date ranges into configurable buckets to manage incremental loads efficiently\n",
    "- **Automatic Retry Mechanism**: Retries failed bucket refreshes with exponential backoff before failing the entire process\n",
    "- **Dataflow Type Auto-Detection**: Automatically detects and supports both regular and CI/CD dataflows\n",
    "- **Connection Management**: Handles database connections with automatic retry and reconnection logic to prevent timeout issues\n",
    "- **Comprehensive Status Tracking**: Maintains detailed metadata about refresh operations in a warehouse tracking table\n",
    "- **Pipeline Failure Integration**: Exits with proper failure codes to integrate with Fabric pipeline error handling\n",
    "\n",
    "## Pipeline Parameters\n",
    "\n",
    "Configure the following parameters when setting up the notebook activity in your Fabric pipeline:\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `workspace_id` | String | Yes | ID of the Fabric workspace containing the dataflow |\n",
    "| `dataflow_id` | String | Yes | ID of the dataflow to refresh |\n",
    "| `dataflow_name` | String | Yes | Name or description of the dataflow |\n",
    "| `initial_load_from_date` | String | Yes* | Start date for initial historical load (format: 'YYYY-MM-DD'). *Required only for first load |\n",
    "| `bucket_size_in_days` | Integer | No | Size of each refresh bucket in days (default: 1) |\n",
    "| `bucket_retry_attempts` | Integer | No | Number of retry attempts for failed buckets (default: 3) |\n",
    "| `incrementally_update_last_n_days` | Integer | No | Number of days to overlap/refresh in incremental updates (default: 1) |\n",
    "| `reinitialize_dataflow` | Boolean | No | Set to True to delete tracking data and restart from scratch (default: False) |\n",
    "| `destination_table` | String | Yes | Name of the destination table in the warehouse where data is written. Can be just table name (uses dbo schema) or `schema.table` format for tables in other schemas |\n",
    "| `incremental_update_column` | String | Yes | DateTime column used for incremental filtering |\n",
    "| `is_cicd_dataflow` | Boolean | No | Explicitly specify if this is a CI/CD dataflow (auto-detected if not provided) |\n",
    "\n",
    "## Notebook Constants\n",
    "\n",
    "The following constants must be configured inside the notebook:\n",
    "\n",
    "| Constant | Example | Description |\n",
    "|----------|---------|-------------|\n",
    "| `SCHEMA` | `\"[Warehouse DB].[dbo]\"` | Database schema where the tracking table resides |\n",
    "| `INCREMENTAL_TABLE` | `\"[Incremental Update]\"` | Name of the metadata tracking table |\n",
    "| `CONNECTION_ARTIFACT` | `\"Warehouse name or id\"` | Name of the warehouse artifact |\n",
    "| `CONNECTION_ARTIFACT_ID` | `\"Workspace id\"` | Technical ID of the warehouse |\n",
    "| `CONNECTION_ARTIFACT_TYPE` | `\"Warehouse\"` | Type of artifact (typically \"Warehouse\") |\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "Follow these steps to set up and configure the notebook:\n",
    "\n",
    "### Step 1: Import the Notebook\n",
    "1. Navigate to your Microsoft Fabric workspace\n",
    "2. Click **New** → **Import notebook**\n",
    "3. Upload the `Incrementally Refresh Dataflow.ipynb` file\n",
    "4. Wait for the import to complete\n",
    "\n",
    "### Step 2: Configure Notebook Constants\n",
    "Open the notebook and update the following constants in the third code cell:\n",
    "\n",
    "```python\n",
    "# Update these constants with your warehouse details\n",
    "SCHEMA = \"[YourWarehouseName].[dbo]\"\n",
    "INCREMENTAL_TABLE = \"[Incremental Update]\"\n",
    "CONNECTION_ARTIFACT = \"YourWarehouseName\"\n",
    "CONNECTION_ARTIFACT_ID = \"your-warehouse-id-guid\"\n",
    "CONNECTION_ARTIFACT_TYPE = \"Warehouse\"\n",
    "```\n",
    "\n",
    "**How to find your Warehouse ID:**\n",
    "1. Open your warehouse in Fabric\n",
    "2. Check the URL: `https://app.fabric.microsoft.com/groups/{workspace_id}/warehouses/{warehouse_id}`\n",
    "3. Copy the `warehouse_id` GUID from the URL\n",
    "\n",
    "### Step 3: Create a Fabric Pipeline\n",
    "1. In your workspace, create a new **Data Pipeline**\n",
    "2. Add a **Notebook** activity to the pipeline canvas\n",
    "3. Configure the notebook activity:\n",
    "   - **Notebook**: Select the imported notebook\n",
    "   - **Parameters**: Add the required parameters (see Quick Start Example below)\n",
    "\n",
    "### Step 4: Configure Your Dataflow\n",
    "Ensure your dataflow Power Query includes logic to read `range_start` and `range_end` from the tracking table (see Integration section).\n",
    "\n",
    "### Step 5: Test the Setup\n",
    "1. Run the pipeline with all required parameters\n",
    "2. Monitor the notebook execution in Fabric\n",
    "3. Check the `[Incremental Update]` table in your warehouse to verify tracking records are created\n",
    "4. Verify data appears in your destination table\n",
    "\n",
    "## Quick Start Example\n",
    "\n",
    "Here's a complete example of how to configure the pipeline parameters for your first run:\n",
    "\n",
    "### Example Configuration\n",
    "\n",
    "**Pipeline Parameters:**\n",
    "```json\n",
    "{\n",
    "  \"workspace_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n",
    "  \"dataflow_id\": \"x9y8z7w6-v5u4-3210-zyxw-vu9876543210\",\n",
    "  \"dataflow_name\": \"Sales Data Incremental Refresh\",\n",
    "  \"initial_load_from_date\": \"2024-01-01\",\n",
    "  \"bucket_size_in_days\": 7,\n",
    "  \"bucket_retry_attempts\": 3,\n",
    "  \"incrementally_update_last_n_days\": 2,\n",
    "  \"reinitialize_dataflow\": false,\n",
    "  \"destination_table\": \"FactSales\",\n",
    "  \"incremental_update_column\": \"OrderDate\",\n",
    "  \"is_cicd_dataflow\": null\n",
    "}\n",
    "```\n",
    "\n",
    "**Parameter Explanation:**\n",
    "- `workspace_id`: Your Fabric workspace GUID (found in workspace URL)\n",
    "- `dataflow_id`: Your dataflow GUID (found in dataflow URL)\n",
    "- `dataflow_name`: Descriptive name for logging/tracking\n",
    "- `initial_load_from_date`: Start loading data from January 1, 2024 (first run only)\n",
    "- `bucket_size_in_days`: Process 7 days at a time\n",
    "- `incrementally_update_last_n_days`: Overlap last 2 days on each refresh\n",
    "- `destination_table`: Table name in warehouse (uses `dbo` schema by default)\n",
    "- `incremental_update_column`: Date column used for filtering\n",
    "- `is_cicd_dataflow`: Auto-detect dataflow type (set to `true` or `false` to override)\n",
    "\n",
    "### Expected First Run Behavior\n",
    "\n",
    "1. **No tracking record exists** → Initial load scenario\n",
    "2. Calculates range: `2024-01-01` to `yesterday 23:59:59`\n",
    "3. Splits into 7-day buckets\n",
    "4. Processes each bucket sequentially\n",
    "5. Creates tracking table entry\n",
    "6. Dataflow reads `range_start` and `range_end` from tracking table\n",
    "7. Data is loaded into `dbo.FactSales` table\n",
    "\n",
    "### Expected Subsequent Runs\n",
    "\n",
    "1. **Tracking record exists** → Incremental update scenario\n",
    "2. Reads last successful range end date\n",
    "3. Calculates new range with 2-day overlap\n",
    "4. Processes new buckets\n",
    "5. Updates tracking table with new range\n",
    "\n",
    "## How It Works\n",
    "\n",
    "### 1. Metadata Table Management\n",
    "\n",
    "The notebook automatically creates and manages an `[Incremental Update]` tracking table in the warehouse with the following schema:\n",
    "\n",
    "- `dataflow_id`, `workspace_id`, `dataflow_name`: Dataflow identifiers\n",
    "- `initial_load_from_date`: Historical start date\n",
    "- `bucket_size_in_days`, `incrementally_update_last_n_days`: Configuration parameters\n",
    "- `destination_table`, `incremental_update_column`: Target table information\n",
    "- `update_time`, `status`: Current refresh status and timestamp\n",
    "- `range_start`, `range_end`: Date range for the current/last refresh bucket\n",
    "- `is_cicd_dataflow`: Flag indicating dataflow type (for API routing)\n",
    "\n",
    "### 2. Dataflow Type Detection\n",
    "\n",
    "The notebook automatically detects whether the dataflow is a CI/CD or regular dataflow by probing the official Microsoft Fabric API endpoints:\n",
    "\n",
    "- **CI/CD Dataflows**: Uses `/v1/workspaces/{workspace_id}/items/{dataflow_id}/jobs/instances` endpoint\n",
    "- **Regular Dataflows**: Uses `/v1.0/myorg/groups/{workspace_id}/dataflows/{dataflow_id}/refreshes` endpoint\n",
    "\n",
    "### 3. Processing Logic\n",
    "\n",
    "#### **Scenario A: Initial Load (No Previous Refresh)**\n",
    "\n",
    "1. Validates that `initial_load_from_date` is provided\n",
    "2. Calculates date range from `initial_load_from_date` to yesterday at 23:59:59\n",
    "3. Splits the date range into buckets based on `bucket_size_in_days`\n",
    "4. For each bucket:\n",
    "   - Deletes any overlapping data in the destination table\n",
    "   - Updates tracking table with \"Running\" status\n",
    "   - Triggers dataflow refresh\n",
    "   - Waits for completion and monitors status\n",
    "   - **If bucket fails**: Retries up to `bucket_retry_attempts` times with exponential backoff (30s, 60s, 120s, etc.)\n",
    "   - **If all retries fail**: Logs error, updates tracking table, and **exits with failure code (1)** to fail the pipeline\n",
    "   - **If bucket succeeds**: Moves to next bucket\n",
    "\n",
    "#### **Scenario B: Previous Refresh Failed**\n",
    "\n",
    "1. Detects failed status from previous run\n",
    "2. Retrieves the failed bucket's date range\n",
    "3. Retries the failed bucket using the same retry logic as above\n",
    "4. **If all retries fail**: Exits with failure code to fail the pipeline\n",
    "5. **If retry succeeds**: Continues with normal incremental processing\n",
    "\n",
    "#### **Scenario C: Incremental Update (Previous Refresh Successful)**\n",
    "\n",
    "1. Calculates new date range:\n",
    "   - If `incrementally_update_last_n_days` is set: Uses `min(last_end_date + 1 second, yesterday - N days)` to ensure overlap without gaps\n",
    "   - Otherwise: Starts from `last_end_date + 1 second`\n",
    "   - End date is always yesterday at 23:59:59\n",
    "2. Splits date range into buckets if needed\n",
    "3. Processes each bucket with the same retry logic as initial load\n",
    "4. **If any bucket fails after all retries**: Exits with failure code to fail the pipeline\n",
    "\n",
    "### 4. Retry Mechanism with Exponential Backoff\n",
    "\n",
    "When a bucket refresh fails, the notebook:\n",
    "\n",
    "1. **Retry 1**: Waits 30 seconds, then retries\n",
    "2. **Retry 2**: Waits 60 seconds, then retries\n",
    "3. **Retry 3**: Waits 120 seconds, then retries\n",
    "4. **If all retries fail**:\n",
    "   - Updates tracking table with failed status\n",
    "   - Logs detailed error message\n",
    "   - Raises `RuntimeError` with failure details\n",
    "   - **Exits with `sys.exit(1)`** to mark the notebook as failed in the pipeline\n",
    "   - **No further buckets are processed**\n",
    "\n",
    "This ensures transient issues (network glitches, temporary service unavailability) are handled gracefully, while persistent failures properly fail the pipeline.\n",
    "\n",
    "### 5. Date Range Logic\n",
    "\n",
    "- **End Date**: Always defaults to yesterday at 23:59:59 (never includes today's partial data)\n",
    "- **Bucket End Times**: Set to 23:59:59 except for the final bucket\n",
    "- **Next Bucket Start**: Previous bucket end + 1 second (ensures no gaps or overlaps)\n",
    "- **Overlap Handling**: When `incrementally_update_last_n_days` is set, the start date ensures overlap while avoiding gaps\n",
    "\n",
    "### 6. Connection Management\n",
    "\n",
    "The notebook proactively manages database connections to prevent timeout issues:\n",
    "\n",
    "- Closes connections before long-running dataflow operations\n",
    "- Validates and recreates connections as needed\n",
    "- Implements retry logic for all database operations (max 3 retries with 1-second delays)\n",
    "\n",
    "## Execution Results\n",
    "\n",
    "Upon completion or failure, the notebook prints:\n",
    "\n",
    "```\n",
    "Dataflow refresh execution completed:\n",
    "Status: Completed / Completed with N failures / Failed: [error message]\n",
    "Total buckets processed: N\n",
    "Successful refreshes: N\n",
    "Failed refreshes: N\n",
    "Total retry attempts: N\n",
    "Duration: X.XX seconds\n",
    "Dataflow type: CI/CD / Regular\n",
    "```\n",
    "\n",
    "If a bucket fails after all retries:\n",
    "\n",
    "```\n",
    "================================================================================\n",
    "DATAFLOW REFRESH FAILED\n",
    "================================================================================\n",
    "Error: Bucket N/M failed after 3 attempts with status Failed. Range: YYYY-MM-DD to YYYY-MM-DD\n",
    "\n",
    "The dataflow refresh has been terminated due to bucket failure after all retry attempts.\n",
    "Please check the logs above for detailed error information.\n",
    "================================================================================\n",
    "```\n",
    "\n",
    "The notebook then exits with code 1, causing the Fabric pipeline to mark the notebook activity as **Failed**.\n",
    "\n",
    "## Integration with Dataflow Power Query\n",
    "\n",
    "Your dataflow Power Query should read the `range_start` and `range_end` parameters from the `[Incremental Update]` table:\n",
    "\n",
    "```powerquery\n",
    "let\n",
    "    Source = Sql.Database(\"[server]\", \"[database]\"),\n",
    "    TrackingTable = Source{[Schema=\"dbo\", Item=\"Incremental Update\"]}[Data],\n",
    "    FilteredRows = Table.SelectRows(TrackingTable, each [dataflow_id] = \"your-dataflow-id\"),\n",
    "    RangeStart = FilteredRows{0}[range_start],\n",
    "    RangeEnd = FilteredRows{0}[range_end],\n",
    "\n",
    "    // Use RangeStart and RangeEnd to filter your data source\n",
    "    FilteredData = Table.SelectRows(YourDataSource,\n",
    "        each [YourDateColumn] >= RangeStart and [YourDateColumn] <= RangeEnd)\n",
    "in\n",
    "    FilteredData\n",
    "```\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Bucket Size**: Start with 1 day buckets. Increase if your data volume is low and performance is not a concern.\n",
    "2. **Retry Attempts**: Default of 3 is recommended. Increase only if you experience frequent transient failures.\n",
    "3. **Overlap Days**: Set `incrementally_update_last_n_days` to 1 or more if your source data can be updated retroactively.\n",
    "4. **Destination Table Schema**:\n",
    "   - If your table is in the `dbo` schema: Use just the table name (e.g., `\"SalesData\"`)\n",
    "   - If your table is in another schema: Use `schema.table` format (e.g., `\"staging.SalesData\"` or `\"analytics.SalesData\"`)\n",
    "5. **Monitoring**: Monitor the `[Incremental Update]` table in your warehouse to track refresh history and troubleshoot issues.\n",
    "6. **Pipeline Design**: Use the notebook activity failure to trigger alerts or retry logic at the pipeline level.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Notebook fails immediately\n",
    "\n",
    "**Symptoms:**\n",
    "- Notebook activity fails within seconds\n",
    "- Error: \"Missing required parameter\" or \"Connection failed\"\n",
    "\n",
    "**Solutions:**\n",
    "1. **Verify all required parameters are provided:**\n",
    "   - `workspace_id`, `dataflow_id`, `dataflow_name`\n",
    "   - `destination_table`, `incremental_update_column`\n",
    "   - `initial_load_from_date` (required for first run only)\n",
    "\n",
    "2. **Check warehouse connection:**\n",
    "   ```sql\n",
    "   -- Test warehouse connection by running this in your warehouse\n",
    "   SELECT TOP 1 * FROM INFORMATION_SCHEMA.TABLES\n",
    "   ```\n",
    "\n",
    "3. **Verify notebook constants are configured:**\n",
    "   - Open the notebook\n",
    "   - Check the third code cell for `SCHEMA`, `CONNECTION_ARTIFACT_ID`, etc.\n",
    "   - Ensure the warehouse ID is correct (copy from warehouse URL)\n",
    "\n",
    "**Common Errors:**\n",
    "- `\"initial_load_from_date is required for the first load\"` → Add this parameter for first execution\n",
    "- `\"Connection timeout\"` → Verify warehouse is running and accessible\n",
    "- `\"Table does not exist\"` → Notebook will auto-create tracking table on first run\n",
    "\n",
    "### Bucket keeps failing after retries\n",
    "\n",
    "**Symptoms:**\n",
    "- Individual buckets fail repeatedly\n",
    "- Error: \"Bucket N/M failed after 3 attempts with status Failed\"\n",
    "- Pipeline marked as failed\n",
    "\n",
    "**Solutions:**\n",
    "1. **Check dataflow execution logs:**\n",
    "   - Open the dataflow in Fabric\n",
    "   - Navigate to **Refresh history**\n",
    "   - Review error messages from failed refreshes\n",
    "\n",
    "2. **Verify Power Query configuration:**\n",
    "   - Ensure dataflow reads `range_start` and `range_end` correctly\n",
    "   - Test with a small date range manually\n",
    "   - Check for data type mismatches\n",
    "\n",
    "3. **Inspect the tracking table:**\n",
    "   ```sql\n",
    "   -- Check current tracking state\n",
    "   SELECT TOP 5\n",
    "       dataflow_id,\n",
    "       dataflow_name,\n",
    "       status,\n",
    "       range_start,\n",
    "       range_end,\n",
    "       update_time\n",
    "   FROM [dbo].[Incremental Update]\n",
    "   WHERE dataflow_id = 'your-dataflow-id'\n",
    "   ORDER BY update_time DESC\n",
    "   ```\n",
    "\n",
    "4. **Reduce bucket size:**\n",
    "   - Try smaller `bucket_size_in_days` (e.g., 1 day instead of 7)\n",
    "   - Large date ranges may timeout or exceed memory limits\n",
    "\n",
    "5. **Check data source:**\n",
    "   - Verify source system is accessible\n",
    "   - Check for connectivity issues during refresh window\n",
    "   - Confirm source data exists for the date range\n",
    "\n",
    "### Wrong dataflow type detected\n",
    "\n",
    "**Symptoms:**\n",
    "- Error: \"Dataflow refresh failed with status 404\" or \"Endpoint not found\"\n",
    "- Notebook detects wrong dataflow type (CI/CD vs Regular)\n",
    "\n",
    "**Solutions:**\n",
    "1. **Explicitly set the dataflow type:**\n",
    "   ```json\n",
    "   {\n",
    "     \"is_cicd_dataflow\": true   // or false for regular dataflows\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Verify workspace and dataflow IDs:**\n",
    "   - Check the URL when viewing your dataflow\n",
    "   - Ensure no typos in the GUID values\n",
    "   - Confirm the dataflow exists in the specified workspace\n",
    "\n",
    "3. **Check dataflow type in Fabric:**\n",
    "   - CI/CD dataflows are typically created through Git integration\n",
    "   - Regular dataflows are created directly in the workspace\n",
    "\n",
    "### Database connection timeouts\n",
    "\n",
    "**Symptoms:**\n",
    "- Error: \"Connection lost\" or \"Idle timeout\"\n",
    "- Random failures during long-running refreshes\n",
    "\n",
    "**Solutions:**\n",
    "- The notebook handles this automatically with retry logic and connection management\n",
    "- Connections are closed before long dataflow operations\n",
    "- If persistent, check:\n",
    "  - Warehouse availability and health status\n",
    "  - Network connectivity between notebook and warehouse\n",
    "  - Fabric capacity resource limits\n",
    "\n",
    "### Tracking table issues\n",
    "\n",
    "**Problem: Tracking table shows \"Running\" but notebook completed**\n",
    "\n",
    "**Solution:**\n",
    "```sql\n",
    "-- Manually check for stuck records\n",
    "SELECT * FROM [dbo].[Incremental Update]\n",
    "WHERE status = 'Running'\n",
    "  AND update_time < DATEADD(HOUR, -2, GETDATE())\n",
    "\n",
    "-- If needed, manually update stuck records\n",
    "UPDATE [dbo].[Incremental Update]\n",
    "SET status = 'Failed'\n",
    "WHERE dataflow_id = 'your-dataflow-id'\n",
    "  AND status = 'Running'\n",
    "  AND update_time < DATEADD(HOUR, -2, GETDATE())\n",
    "```\n",
    "\n",
    "**Problem: Need to restart from scratch**\n",
    "\n",
    "**Solution:**\n",
    "```sql\n",
    "-- Option 1: Delete tracking record (will trigger initial load on next run)\n",
    "DELETE FROM [dbo].[Incremental Update]\n",
    "WHERE dataflow_id = 'your-dataflow-id'\n",
    "\n",
    "-- Option 2: Use reinitialize_dataflow parameter\n",
    "-- Set reinitialize_dataflow = true in pipeline parameters\n",
    "```\n",
    "\n",
    "### Monitoring and debugging\n",
    "\n",
    "**Check tracking table history:**\n",
    "```sql\n",
    "-- View refresh history\n",
    "SELECT\n",
    "    dataflow_name,\n",
    "    status,\n",
    "    range_start,\n",
    "    range_end,\n",
    "    update_time,\n",
    "    DATEDIFF(SECOND,\n",
    "        LAG(update_time) OVER (PARTITION BY dataflow_id ORDER BY update_time),\n",
    "        update_time\n",
    "    ) as seconds_since_last_refresh\n",
    "FROM [dbo].[Incremental Update]\n",
    "WHERE dataflow_id = 'your-dataflow-id'\n",
    "ORDER BY update_time DESC\n",
    "```\n",
    "\n",
    "**Check for data gaps:**\n",
    "```sql\n",
    "-- Identify gaps in refreshed date ranges\n",
    "WITH RangedData AS (\n",
    "    SELECT\n",
    "        range_start,\n",
    "        range_end,\n",
    "        LEAD(range_start) OVER (ORDER BY range_start) as next_start\n",
    "    FROM [dbo].[Incremental Update]\n",
    "    WHERE dataflow_id = 'your-dataflow-id'\n",
    "      AND status = 'Success'\n",
    ")\n",
    "SELECT\n",
    "    range_end as gap_start,\n",
    "    next_start as gap_end,\n",
    "    DATEDIFF(DAY, range_end, next_start) as gap_days\n",
    "FROM RangedData\n",
    "WHERE DATEADD(SECOND, 1, range_end) < next_start\n",
    "```\n",
    "\n",
    "### Getting help\n",
    "\n",
    "If you continue experiencing issues:\n",
    "1. Check the notebook execution logs in Fabric\n",
    "2. Review the dataflow refresh history\n",
    "3. Verify all configuration values match your environment\n",
    "4. Test with a minimal date range (1-2 days) first\n",
    "\n",
    "## Version History\n",
    "\n",
    "- **v3.0**: Added bucket retry mechanism with exponential backoff and pipeline failure integration\n",
    "- **v2.0**: Added CI/CD dataflow support with automatic type detection\n",
    "- **v1.0**: Initial implementation with basic incremental refresh framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14028713-9697-4566-824a-aef63c4d6f55",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters passed from the pipeline as base parameters of the notebook activity\n",
    "# Workspace id\n",
    "workspace_id = ''\n",
    "\n",
    "# Dataflow id\n",
    "dataflow_id = ''\n",
    "\n",
    "# Dataflow name\n",
    "dataflow_name = ''\n",
    "\n",
    "# Initial load from date for the first load\n",
    "initial_load_from_date = ''\n",
    "\n",
    "# Bucket size for each load\n",
    "bucket_size_in_days = 1\n",
    "\n",
    "# Number of retry attempts for failed bucket refreshes\n",
    "bucket_retry_attempts = 3\n",
    "\n",
    "# Reinitialize dataflow\n",
    "reinitialize_dataflow = False\n",
    "\n",
    "# Incrementally update last n days\n",
    "incrementally_update_last_n_days = 1\n",
    "\n",
    "# Destination table\n",
    "destination_table = ''\n",
    "\n",
    "# Incremental update column\n",
    "incremental_update_column = ''\n",
    "\n",
    "# Indicates if this is a CI/CD dataflow (optional, auto-detected if not specified)\n",
    "is_cicd_dataflow = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b53d9c-fc4e-4f85-b08f-f00e5fb8c6a9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import sempy.fabric as fabric\n",
    "from typing import Optional, Dict, Any, Union, Tuple, List\n",
    "\n",
    "# Constants\n",
    "SCHEMA = \"[Warehouse DB].[dbo]\"\n",
    "INCREMENTAL_TABLE = \"[Incremental Update]\"\n",
    "CONNECTION_ARTIFACT = \"Warehouse name or id\"\n",
    "CONNECTION_ARTIFACT_ID = \"Workspace id\"\n",
    "CONNECTION_ARTIFACT_TYPE = \"Warehouse\"\n",
    "\n",
    "class DataflowRefresher:\n",
    "    \"\"\"\n",
    "    Class to incrementally refresh a dataflow in Microsoft Fabric\n",
    "    with support for initial loads and incremental updates\n",
    "    Enhanced to support both regular dataflow gen2 and dataflow gen2 CI/CD objects\n",
    "    using official Microsoft Fabric APIs\n",
    "    \"\"\"\n",
    "    def __init__(self, client, artifact: str, artifact_id: str, artifact_type: str,\n",
    "                 schema: str, incremental_table: str, log_level: int):\n",
    "        \"\"\"\n",
    "        Initialize the DataflowRefresher\n",
    "        \n",
    "        Args:\n",
    "            client: API client for Microsoft Fabric\n",
    "            artifact: Name of the artifact to connect to (e.g., \"The Beer Store\")\n",
    "            artifact_id: ID of the artifact (e.g., workspace ID)\n",
    "            artifact_type: Type of the artifact (e.g., \"Warehouse\")\n",
    "            schema: Database schema name including brackets, e.g. \"[The Beer Store].[dbo]\"\n",
    "            incremental_table: Name of the table tracking incremental updates (without schema)\n",
    "            log_level: level of logging\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.artifact = artifact\n",
    "        self.artifact_id = artifact_id\n",
    "        self.artifact_type = artifact_type\n",
    "        self.connection = None\n",
    "        self.schema = schema\n",
    "        self.incremental_table = f\"{schema}.{incremental_table}\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(log_level)\n",
    "        self._ensure_connection()\n",
    "        self.create_incremental()\n",
    "    \n",
    "    def _create_connection(self):\n",
    "        \"\"\"\n",
    "        Create a new connection to the artifact\n",
    "        \n",
    "        Returns:\n",
    "            A new database connection\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import notebookutils.data\n",
    "            return notebookutils.data.connect_to_artifact(\n",
    "                self.artifact, self.artifact_id, self.artifact_type)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating connection: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _ensure_connection(self):\n",
    "        \"\"\"\n",
    "        Ensure that we have a valid database connection, creating a new one if needed\n",
    "        \n",
    "        Returns:\n",
    "            A valid database connection\n",
    "        \"\"\"\n",
    "        if self.connection is None:\n",
    "            self.connection = self._create_connection()\n",
    "            return self.connection\n",
    "            \n",
    "        # Test if the existing connection is still valid\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(\"SELECT 1\")\n",
    "            cursor.fetchall()\n",
    "            cursor.close()\n",
    "            return self.connection\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Connection test failed, reconnecting: {e}\")\n",
    "            self._close_connection()\n",
    "            self.connection = self._create_connection()\n",
    "            return self.connection\n",
    "    \n",
    "    def _close_connection(self):\n",
    "        \"\"\"Close the current connection if it exists\"\"\"\n",
    "        if self.connection is not None:\n",
    "            try:\n",
    "                self.connection.close()\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error closing connection: {e}\")\n",
    "            finally:\n",
    "                self.connection = None\n",
    "    \n",
    "    def _execute_with_retry(self, sql, params=None, commit=True, max_retries=3):\n",
    "        \"\"\"\n",
    "        Execute a SQL statement with automatic retry on connection issues\n",
    "        \n",
    "        Args:\n",
    "            sql: SQL statement to execute\n",
    "            params: Parameters for the SQL statement\n",
    "            commit: Whether to commit the transaction\n",
    "            max_retries: Maximum number of retries\n",
    "            \n",
    "        Returns:\n",
    "            Database cursor\n",
    "        \"\"\"\n",
    "        retries = 0\n",
    "        last_error = None\n",
    "        \n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                # Ensure we have a valid connection\n",
    "                self._ensure_connection()\n",
    "                \n",
    "                # Execute the SQL\n",
    "                cursor = self.connection.execute(sql, params or ())\n",
    "                \n",
    "                # Commit if requested\n",
    "                if commit:\n",
    "                    self.connection.commit()\n",
    "                    \n",
    "                return cursor\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                self.logger.warning(f\"Database operation failed (attempt {retries+1}/{max_retries}): {e}\")\n",
    "                self._close_connection()  # Force reconnection on next attempt\n",
    "                retries += 1\n",
    "                \n",
    "                # Small delay before retry\n",
    "                if retries < max_retries:\n",
    "                    time.sleep(1)\n",
    "        \n",
    "        # If we get here, we've exhausted retries\n",
    "        self.logger.error(f\"Database operation failed after {max_retries} attempts: {last_error}\")\n",
    "        raise last_error\n",
    "    \n",
    "    def create_incremental(self) -> bool:\n",
    "        \"\"\"\n",
    "        Create the incremental update meta data table if it does not exist\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sql = f\"\"\"\n",
    "                IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'Incremental Update' AND schema_id = SCHEMA_ID('dbo'))\n",
    "                BEGIN\n",
    "                CREATE TABLE {self.incremental_table}\n",
    "                (\n",
    "                    [dataflow_id] [varchar](60) NOT NULL,\n",
    "                    [workspace_id] [VARCHAR](60) NOT NULL,\n",
    "                    [dataflow_name] [varchar](60) NULL,\n",
    "                    [initial_load_from_date] [datetime2](3) NOT NULL,\n",
    "                    [bucket_size_in_days] [int] NOT NULL,\n",
    "                    [incrementally_update_last_n_days] [int] NOT NULL,\n",
    "                    [destination_table] [VARCHAR](60) NOT NULL,\n",
    "                    [incremental_update_column] [VARCHAR](60) NOT NULL,\n",
    "                    [update_time] [datetime2](3) NULL,\n",
    "                    [status] [varchar](50) NOT NULL,\n",
    "                    [range_start] [datetime2](3) NULL,\n",
    "                    [range_end] [datetime2](3) NULL,\n",
    "                    [is_cicd_dataflow] [bit] NULL\n",
    "                )\n",
    "                END\n",
    "                ELSE\n",
    "                BEGIN\n",
    "                    -- Add the new column if it doesn't exist (for backward compatibility)\n",
    "                    IF NOT EXISTS (SELECT * FROM sys.columns WHERE object_id = OBJECT_ID('{self.incremental_table}') AND name = 'is_cicd_dataflow')\n",
    "                    BEGIN\n",
    "                        ALTER TABLE {self.incremental_table} ADD [is_cicd_dataflow] [bit] NULL\n",
    "                    END\n",
    "                END\n",
    "            \"\"\"\n",
    "            self._execute_with_retry(sql)\n",
    "            self.logger.info(f\"Successfully created/updated incremental update table in the database\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating/updating incremental refresh table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _detect_dataflow_type(self, workspace_id: str, dataflow_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Auto-detect if a dataflow is a CI/CD dataflow by checking the official endpoints\n",
    "        \n",
    "        Args:\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_id: ID of the dataflow\n",
    "            \n",
    "        Returns:\n",
    "            True if CI/CD dataflow, False if regular dataflow\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try the official CI/CD endpoint first\n",
    "            try:\n",
    "                endpoint = f\"/v1/workspaces/{workspace_id}/items/{dataflow_id}\"\n",
    "                response = self.client.get(endpoint)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    item_info = response.json()\n",
    "                    # Check if it's a dataflow and has CI/CD characteristics\n",
    "                    item_type = item_info.get(\"type\", \"\").lower()\n",
    "                    if item_type == \"dataflow\" or \"dataflow\" in item_type:\n",
    "                        self.logger.info(f\"Detected CI/CD dataflow for {dataflow_id}\")\n",
    "                        return True\n",
    "                        \n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"CI/CD endpoint check failed: {e}\")\n",
    "            \n",
    "            # Try regular dataflow endpoint\n",
    "            try:\n",
    "                endpoint = f\"/v1.0/myorg/groups/{workspace_id}/dataflows/{dataflow_id}\"\n",
    "                response = self.client.get(endpoint)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    self.logger.info(f\"Detected regular dataflow for {dataflow_id}\")\n",
    "                    return False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"Regular dataflow endpoint check failed: {e}\")\n",
    "                \n",
    "            # Default to regular dataflow if detection fails\n",
    "            self.logger.warning(f\"Could not auto-detect dataflow type for {dataflow_id}, defaulting to regular dataflow\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error during dataflow type detection: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_incremental(self, dataflow_id: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the last refresh details from the tracking table\n",
    "        \n",
    "        Args:\n",
    "            dataflow_id: ID of the dataflow\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with the last refresh details or None if no records found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sql = f\"\"\"\n",
    "                SELECT TOP (1) [dataflow_id],\n",
    "                         [update_time],\n",
    "                         [status],\n",
    "                         [range_start],\n",
    "                         [range_end],\n",
    "                         [is_cicd_dataflow]\n",
    "                FROM {self.incremental_table}\n",
    "                WHERE [dataflow_id] = ?\n",
    "                ORDER BY [update_time] DESC\n",
    "            \"\"\"\n",
    "            cursor = self._execute_with_retry(sql, (dataflow_id,), commit=False)\n",
    "            columns = [column[0] for column in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            data = [tuple(row) for row in data]\n",
    "            \n",
    "            if not data:\n",
    "                self.logger.info(f\"No previous refresh records found for dataflow {dataflow_id}\")\n",
    "                return None\n",
    "                \n",
    "            return pd.DataFrame(data, columns=columns)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting incremental refresh data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def insert_into_incremental(self, dataflow_id: str, workspace_id: str, dataflow_name: str, initial_load_from_date: str,\n",
    "                            bucket_size_in_days: int, incrementally_update_last_n_days: int, destination_table: str,\n",
    "                            incremental_update_column: str, status: str, \n",
    "                            range_start: datetime, range_end: datetime, is_cicd_dataflow: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        Insert into incremental table in the warehouse\n",
    "        \n",
    "        Args:\n",
    "            dataflow_id: ID of the dataflow\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_name: Name or description of the dataflow\n",
    "            initial_load_from_date: Initial load from date\n",
    "            bucket_size_in_days: Bucket size of each refresh in days\n",
    "            incrementally_update_last_n_days: Incremental refresh bucket size\n",
    "            destination_table: Destination table in the warehouse where data is written\n",
    "            incremental_update_column: Column of the destination table used for incremental update\n",
    "            status: Status of the refresh operation\n",
    "            range_start: Start of the refresh date range\n",
    "            range_end: End of the refresh date range\n",
    "            is_cicd_dataflow: Whether this is a CI/CD dataflow\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            start_formatted = range_start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            end_formatted = range_end.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            \n",
    "            sql = f\"\"\"\n",
    "                INSERT INTO {self.incremental_table}\n",
    "                (\n",
    "                    [dataflow_id], \n",
    "                    [workspace_id],\n",
    "                    [dataflow_name],\n",
    "                    [initial_load_from_date],\n",
    "                    [bucket_size_in_days],\n",
    "                    [incrementally_update_last_n_days],\n",
    "                    [destination_table],\n",
    "                    [incremental_update_column],\n",
    "                    [update_time],\n",
    "                    [status],\n",
    "                    [range_start],\n",
    "                    [range_end],\n",
    "                    [is_cicd_dataflow]\n",
    "                )\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            params = (dataflow_id, workspace_id, dataflow_name, initial_load_from_date, \n",
    "                     bucket_size_in_days, incrementally_update_last_n_days, destination_table, \n",
    "                     incremental_update_column, current_time, status, start_formatted, end_formatted, is_cicd_dataflow)\n",
    "            \n",
    "            self._execute_with_retry(sql, params)\n",
    "            self.logger.info(f\"Successfully inserted record for dataflow {dataflow_id}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error inserting into incremental table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_incremental(self, dataflow_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete the entry from the tracking table\n",
    "        \n",
    "        Args:\n",
    "            dataflow_id: ID of the dataflow\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sql = f\"\"\"\n",
    "                DELETE\n",
    "                FROM {self.incremental_table}\n",
    "                WHERE [dataflow_id] = ?\n",
    "            \"\"\"\n",
    "            self._execute_with_retry(sql, (dataflow_id,))\n",
    "            self.logger.info(f\"Successfully deleted old record for dataflow {dataflow_id}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Could not delete old record for dataflow: {e}\")\n",
    "            raise\n",
    "\n",
    "    def update_incremental(self, dataflow_id: str, status: str, \n",
    "                        range_start: datetime, range_end: datetime) -> bool:\n",
    "        \"\"\"\n",
    "        Update incremental table in the warehouse with the new parameters\n",
    "        \n",
    "        Args:\n",
    "            dataflow_id: ID of the dataflow\n",
    "            status: Status of the refresh operation\n",
    "            range_start: Start of the refresh date range\n",
    "            range_end: End of the refresh date range\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            start_formatted = range_start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            end_formatted = range_end.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "            \n",
    "            sql = f\"\"\"\n",
    "                UPDATE {self.incremental_table}\n",
    "                SET \n",
    "                [status] = ?,\n",
    "                [update_time] = ?,\n",
    "                [range_start] = ?,\n",
    "                [range_end] = ?\n",
    "                WHERE [dataflow_id] = ?\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor = self._execute_with_retry(sql, (status, current_time, \n",
    "                                               start_formatted, end_formatted, dataflow_id))\n",
    "            rows_affected = cursor.rowcount\n",
    "            \n",
    "            if rows_affected > 0:\n",
    "                self.logger.info(f\"Successfully updated record for dataflow {dataflow_id}\")\n",
    "                return True\n",
    "            else:\n",
    "                self.logger.warning(f\"No records updated for dataflow {dataflow_id}, record may not exist\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error updating incremental table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_data(self, table: str, column: str,\n",
    "                 range_start: Union[datetime, str],\n",
    "                 range_end: Union[datetime, str],\n",
    "                 dataflow_id: Optional[str] = None) -> int:\n",
    "        \"\"\"\n",
    "        Delete any overlapping data from the previous refreshes\n",
    "\n",
    "        Args:\n",
    "            table: Target table name (can be schema.table or just table, defaults to dbo schema)\n",
    "            column: Date column to use for filtering\n",
    "            range_start: Start of the date range to delete (datetime or string)\n",
    "            range_end: End of the date range to delete (datetime or string)\n",
    "            dataflow_id: Optional ID of the dataflow for logging purposes\n",
    "\n",
    "        Returns:\n",
    "            Number of rows deleted\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Format dates if they're datetime objects\n",
    "            start_formatted = range_start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] if isinstance(range_start, datetime) else range_start\n",
    "            end_formatted = range_end.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] if isinstance(range_end, datetime) else range_end\n",
    "\n",
    "            # Parse schema and table name\n",
    "            if '.' in table:\n",
    "                # Schema.Table format provided\n",
    "                schema_name, table_name = table.split('.', 1)\n",
    "                full_table_name = f\"[The Beer Store].[{schema_name}].[{table_name}]\"\n",
    "            else:\n",
    "                # Only table name provided, use default dbo schema\n",
    "                full_table_name = f\"{self.schema}.[{table}]\"\n",
    "\n",
    "            sql = f\"\"\"\n",
    "                DELETE FROM {full_table_name}\n",
    "                WHERE [{column}] BETWEEN ? AND ?\n",
    "            \"\"\"\n",
    "\n",
    "            cursor = self._execute_with_retry(sql, (start_formatted, end_formatted))\n",
    "            rows_deleted = cursor.rowcount\n",
    "\n",
    "            log_id = f\" for {dataflow_id}\" if dataflow_id else \"\"\n",
    "            self.logger.info(f\"Successfully deleted {rows_deleted} overlapping records{log_id} in table {table} between {start_formatted} and {end_formatted}\")\n",
    "\n",
    "            return rows_deleted\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error deleting overlapping records: {e}\")\n",
    "            raise\n",
    "\n",
    "    def refresh_dataflow(self, workspace_id: str, dataflow_id: str, dataflow_name: str,\n",
    "                        is_cicd_dataflow: bool = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Refresh dataflow and return the response\n",
    "        Enhanced to support both regular and CI/CD dataflows using official Microsoft APIs\n",
    "        \n",
    "        Args:\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_id: ID of the dataflow\n",
    "            dataflow_name: Name of the dataflow\n",
    "            is_cicd_dataflow: Whether this is a CI/CD dataflow (auto-detected if None)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the response from the refresh API\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Auto-detect if not specified\n",
    "            if is_cicd_dataflow is None:\n",
    "                is_cicd_dataflow = self._detect_dataflow_type(workspace_id, dataflow_id)\n",
    "            \n",
    "            if is_cicd_dataflow:\n",
    "                # Official Microsoft Fabric API for CI/CD dataflows - simplified without user parameters\n",
    "                endpoint = f\"/v1/workspaces/{workspace_id}/items/{dataflow_id}/jobs/instances?jobType=Refresh\"\n",
    "                payload = {\n",
    "                    \"executionData\": {\n",
    "                        \"DataflowName\": dataflow_name\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                self.logger.info(f\"Using official CI/CD endpoint for dataflow {dataflow_id}\")\n",
    "                \n",
    "                try:\n",
    "                    response = self.client.post(endpoint, json=payload)\n",
    "                    self.logger.info(f\"Successfully triggered CI/CD refresh for dataflow {dataflow_id}\")\n",
    "                    return response\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Official CI/CD endpoint failed, trying fallback: {e}\")\n",
    "                    # Fallback to regular dataflow endpoint\n",
    "                    is_cicd_dataflow = False\n",
    "            \n",
    "            if not is_cicd_dataflow:\n",
    "                # Regular dataflow gen2 API\n",
    "                endpoint = f\"/v1.0/myorg/groups/{workspace_id}/dataflows/{dataflow_id}/refreshes\"\n",
    "                payload = {\"refreshRequest\": \"y\"}\n",
    "                \n",
    "                response = self.client.post(endpoint, json=payload)\n",
    "                self.logger.info(f\"Successfully triggered regular refresh for dataflow {dataflow_id}\")\n",
    "                return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error triggering refresh: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_latest_refresh_status(self, workspace_id: str, dataflow_id: str, is_cicd_dataflow: bool = None) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get refresh status of a dataflow\n",
    "        Enhanced to support both regular and CI/CD dataflows using official Microsoft APIs\n",
    "        \n",
    "        Args:\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_id: ID of the dataflow\n",
    "            is_cicd_dataflow: Whether this is a CI/CD dataflow (auto-detected if None)\n",
    "            \n",
    "        Returns:\n",
    "            Status of the latest refresh operation or None if not available\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Auto-detect if not specified\n",
    "            if is_cicd_dataflow is None:\n",
    "                is_cicd_dataflow = self._detect_dataflow_type(workspace_id, dataflow_id)\n",
    "            \n",
    "            if is_cicd_dataflow:\n",
    "                # Official Microsoft Fabric API for CI/CD dataflow status\n",
    "                endpoint = f\"/v1/workspaces/{workspace_id}/items/{dataflow_id}/jobs/instances\"\n",
    "                \n",
    "                try:\n",
    "                    response = self.client.get(endpoint)\n",
    "                    data = response.json()\n",
    "                    \n",
    "                    # The response should contain job instances\n",
    "                    if 'value' in data and len(data['value']) > 0:\n",
    "                        # Get the most recent job instance\n",
    "                        latest_job = data['value'][0]  # Assuming sorted by most recent\n",
    "                        status = latest_job.get(\"status\", \"Unknown\")\n",
    "                        self.logger.info(f\"Latest CI/CD refresh status for dataflow {dataflow_id}: {status}\")\n",
    "                        return status\n",
    "                    elif isinstance(data, list) and len(data) > 0:\n",
    "                        latest_job = data[0]\n",
    "                        status = latest_job.get(\"status\", \"Unknown\")\n",
    "                        self.logger.info(f\"Latest CI/CD refresh status for dataflow {dataflow_id}: {status}\")\n",
    "                        return status\n",
    "                    else:\n",
    "                        self.logger.warning(f\"No CI/CD job instances found for dataflow {dataflow_id}\")\n",
    "                        return None\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"CI/CD status endpoint failed, trying regular endpoint: {e}\")\n",
    "                    # Fallback to regular dataflow endpoint\n",
    "                    is_cicd_dataflow = False\n",
    "            \n",
    "            if not is_cicd_dataflow:\n",
    "                # Regular dataflow gen2 API\n",
    "                endpoint = f\"/v1.0/myorg/groups/{workspace_id}/dataflows/{dataflow_id}/transactions\"\n",
    "                \n",
    "                response = self.client.get(endpoint)\n",
    "                data = response.json()\n",
    "                \n",
    "                if 'value' in data and len(data['value']) > 0:\n",
    "                    latest_transaction = data['value'][0]\n",
    "                    status = latest_transaction.get(\"status\", \"Unknown\")\n",
    "                    self.logger.info(f\"Latest regular refresh status for dataflow {dataflow_id}: {status}\")\n",
    "                    return status\n",
    "                else:\n",
    "                    self.logger.warning(f\"No refresh transactions found for dataflow {dataflow_id}\")\n",
    "                    return None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking refresh status: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def wait_for_refresh_completion(self, workspace_id: str, dataflow_id: str, \n",
    "                                 timeout_minutes: int = 60, \n",
    "                                 check_interval_seconds: int = 30,\n",
    "                                 is_cicd_dataflow: bool = None) -> str:\n",
    "        \"\"\"\n",
    "        Wait for a dataflow refresh to complete\n",
    "        Enhanced to support both regular and CI/CD dataflows\n",
    "        \n",
    "        Args:\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_id: ID of the dataflow\n",
    "            timeout_minutes: Maximum wait time in minutes\n",
    "            check_interval_seconds: Interval between status checks in seconds\n",
    "            is_cicd_dataflow: Whether this is a CI/CD dataflow (auto-detected if None)\n",
    "            \n",
    "        Returns:\n",
    "            Final status of the refresh operation\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Waiting for dataflow {dataflow_id} refresh to complete (timeout: {timeout_minutes} minutes)\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        timeout = timedelta(minutes=timeout_minutes)\n",
    "        \n",
    "        while datetime.now() - start_time < timeout:\n",
    "            # Close existing connection before potentially long wait\n",
    "            # This is critical to avoid idle timeout issues\n",
    "            self._close_connection()\n",
    "            \n",
    "            status = self.get_latest_refresh_status(workspace_id, dataflow_id, is_cicd_dataflow)\n",
    "            \n",
    "            if status in [\"Success\", \"Failed\", \"Cancelled\", \"Completed\", \"Error\", \"Succeeded\"]:\n",
    "                self.logger.info(f\"Dataflow refresh completed with status: {status}\")\n",
    "                return status\n",
    "                \n",
    "            self.logger.info(f\"Current status: {status}, checking again in {check_interval_seconds} seconds\")\n",
    "            time.sleep(check_interval_seconds)\n",
    "            \n",
    "        self.logger.warning(f\"Refresh timeout reached after {timeout_minutes} minutes\")\n",
    "        return \"Timeout\"\n",
    "\n",
    "    def _parse_date(self, date_str: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Parse a date string into a datetime object\n",
    "        \n",
    "        Args:\n",
    "            date_str: Date string in various formats\n",
    "            \n",
    "        Returns:\n",
    "            Datetime object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try different formats\n",
    "            for fmt in ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S']:\n",
    "                try:\n",
    "                    return datetime.strptime(date_str, fmt)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # If all formats fail, raise exception\n",
    "            raise ValueError(f\"Unable to parse date string: {date_str}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing date: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _get_date_ranges(self, start_date: datetime, end_date: datetime, bucket_size_days: int) -> List[Tuple[datetime, datetime]]:\n",
    "        \"\"\"\n",
    "        Split a date range into smaller buckets\n",
    "        \n",
    "        Args:\n",
    "            start_date: Start date of the range\n",
    "            end_date: End date of the range\n",
    "            bucket_size_days: Size of each bucket in days\n",
    "            \n",
    "        Returns:\n",
    "            List of (start_date, end_date) tuples for each bucket\n",
    "        \"\"\"\n",
    "        date_ranges = []\n",
    "        current_start = start_date\n",
    "        \n",
    "        while current_start < end_date:\n",
    "            current_end = min(current_start + timedelta(days=bucket_size_days), end_date)\n",
    "            \n",
    "            # Set time to 23:59:59 for the end date of each bucket except the last one\n",
    "            if current_end < end_date:\n",
    "                current_end = datetime(current_end.year, current_end.month, current_end.day, 23, 59, 59)\n",
    "                \n",
    "            date_ranges.append((current_start, current_end))\n",
    "            \n",
    "            # Start the next bucket from the day after the current end\n",
    "            current_start = current_end + timedelta(seconds=1)\n",
    "        \n",
    "        return date_ranges\n",
    "\n",
    "    def execute_incremental_refresh(self,\n",
    "                               workspace_id: str,\n",
    "                               dataflow_id: str,\n",
    "                               dataflow_name: str,\n",
    "                               destination_table: str,\n",
    "                               incremental_update_column: str,\n",
    "                               initial_load_from_date: str = None,\n",
    "                               bucket_size_in_days: int = 30,\n",
    "                               reinitialize_dataflow: bool = False,\n",
    "                               incrementally_update_last_n_days: int = None,\n",
    "                               wait_for_completion: bool = True,\n",
    "                               timeout_minutes: int = 120,\n",
    "                               is_cicd_dataflow: bool = None,\n",
    "                               bucket_retry_attempts: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute a complete incremental refresh workflow following the specified logic\n",
    "        Enhanced to support both regular and CI/CD dataflows\n",
    "\n",
    "        Args:\n",
    "            workspace_id: ID of the workspace\n",
    "            dataflow_id: ID of the dataflow\n",
    "            dataflow_name: Name or description of the dataflow\n",
    "            destination_table: Name of the destination table\n",
    "            incremental_update_column: Column used for incremental updates\n",
    "            initial_load_from_date: Start date for the initial load (required for first load)\n",
    "            bucket_size_in_days: Size of each refresh bucket in days\n",
    "            reinitialize_dataflow: Whether to reinitialize the dataflow\n",
    "            incrementally_update_last_n_days: Number of days to update incrementally\n",
    "            wait_for_completion: Whether to wait for refresh completion\n",
    "            timeout_minutes: Timeout when waiting for completion\n",
    "            is_cicd_dataflow: Whether this is a CI/CD dataflow (auto-detected if None)\n",
    "            bucket_retry_attempts: Number of times to retry a failed bucket before moving to next\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing execution results and statistics\n",
    "        \"\"\"\n",
    "        # Ensure we have a fresh connection at the start\n",
    "        self._ensure_connection()\n",
    "        \n",
    "        # Auto-detect dataflow type if not specified\n",
    "        if is_cicd_dataflow is None:\n",
    "            is_cicd_dataflow = self._detect_dataflow_type(workspace_id, dataflow_id)\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        self.logger.info(f\"Starting incremental refresh for {'CI/CD ' if is_cicd_dataflow else ''}dataflow {dataflow_id}\")\n",
    "        \n",
    "        # Get yesterday's date at 23:59:59 as the default end date\n",
    "        yesterday = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\n",
    "        yesterday_end = datetime(yesterday.year, yesterday.month, yesterday.day, 23, 59, 59)\n",
    "        \n",
    "        # Initialize statistics\n",
    "        stats = {\n",
    "            \"dataflow_id\": dataflow_id,\n",
    "            \"start_time\": start_time,\n",
    "            \"buckets_refreshed\": 0,\n",
    "            \"successful_refreshes\": 0,\n",
    "            \"failed_refreshes\": 0,\n",
    "            \"total_retry_attempts\": 0,\n",
    "            \"current_status\": \"Started\",\n",
    "            \"is_cicd_dataflow\": is_cicd_dataflow\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check if we need to reinitialize the dataflow\n",
    "            if reinitialize_dataflow:\n",
    "                self.logger.info(f\"Reinitializing dataflow {dataflow_id} as requested\")\n",
    "                self.delete_incremental(dataflow_id)\n",
    "                last_refresh_data = None\n",
    "            else:\n",
    "                # Get the last refresh record\n",
    "                last_refresh_df = self.get_incremental(dataflow_id)\n",
    "                last_refresh_data = last_refresh_df.iloc[0].to_dict() if last_refresh_df is not None else None\n",
    "            \n",
    "            # Case 1: No previous refresh or reinitializing dataflow\n",
    "            if last_refresh_data is None:\n",
    "                self.logger.info(f\"No previous refresh found or reinitializing for dataflow {dataflow_id}\")\n",
    "                \n",
    "                if not initial_load_from_date:\n",
    "                    raise ValueError(\"initial_load_from_date is required for the first load\")\n",
    "                \n",
    "                start_date = self._parse_date(initial_load_from_date)\n",
    "                end_date = yesterday_end\n",
    "                \n",
    "                self.logger.info(f\"Performing initial load from {start_date} to {end_date}\")\n",
    "                \n",
    "                # Split the date range into buckets\n",
    "                date_ranges = self._get_date_ranges(start_date, end_date, bucket_size_in_days)\n",
    "                \n",
    "                stats[\"date_ranges\"] = len(date_ranges)\n",
    "                stats[\"start_date\"] = start_date\n",
    "                stats[\"end_date\"] = end_date\n",
    "                \n",
    "                for i, (range_start, range_end) in enumerate(date_ranges):\n",
    "                    self.logger.info(f\"Processing bucket {i+1}/{len(date_ranges)}: {range_start} to {range_end}\")\n",
    "\n",
    "                    # Retry loop for this bucket\n",
    "                    bucket_success = False\n",
    "                    for attempt in range(bucket_retry_attempts):\n",
    "                        if attempt > 0:\n",
    "                            self.logger.info(f\"Retry attempt {attempt}/{bucket_retry_attempts-1} for bucket {i+1}/{len(date_ranges)}\")\n",
    "                            stats[\"total_retry_attempts\"] += 1\n",
    "                            # Exponential backoff: wait 30, 60, 120 seconds, etc.\n",
    "                            wait_time = 30 * (2 ** (attempt - 1))\n",
    "                            self.logger.info(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                            time.sleep(wait_time)\n",
    "\n",
    "                        # Delete data in the range\n",
    "                        self.delete_data(destination_table, incremental_update_column, range_start, range_end, dataflow_id)\n",
    "\n",
    "                        # Insert a record with \"Running\" status\n",
    "                        if i == 0 and attempt == 0:\n",
    "                            self.insert_into_incremental(dataflow_id, workspace_id, dataflow_name, initial_load_from_date,\n",
    "                                            bucket_size_in_days, incrementally_update_last_n_days, destination_table,\n",
    "                                            incremental_update_column, \"Running\", range_start, range_end, is_cicd_dataflow)\n",
    "                        else:\n",
    "                            self.update_incremental(dataflow_id, \"Running\", range_start, range_end)\n",
    "\n",
    "                        # Close connection before long-running dataflow operation\n",
    "                        self._close_connection()\n",
    "\n",
    "                        # Trigger dataflow refresh\n",
    "                        self.refresh_dataflow(workspace_id, dataflow_id, dataflow_name, is_cicd_dataflow)\n",
    "\n",
    "                        # Wait for completion if requested\n",
    "                        if wait_for_completion:\n",
    "                            # Already closed connection before this step\n",
    "                            # Add a small buffer of 5 seconds just to make sure the status is updated\n",
    "                            time.sleep(5)\n",
    "                            status = self.wait_for_refresh_completion(workspace_id, dataflow_id, timeout_minutes, 30, is_cicd_dataflow)\n",
    "\n",
    "                            # Ensure fresh connection for database operations\n",
    "                            self._ensure_connection()\n",
    "\n",
    "                            # Update status in the incremental table\n",
    "                            self.update_incremental(dataflow_id, status, range_start, range_end)\n",
    "\n",
    "                            if status in [\"Success\", \"Succeeded\", \"Completed\"]:\n",
    "                                bucket_success = True\n",
    "                                stats[\"successful_refreshes\"] += 1\n",
    "                                self.logger.info(f\"Bucket {i+1}/{len(date_ranges)} completed successfully\")\n",
    "                                break  # Exit retry loop on success\n",
    "                            else:\n",
    "                                self.logger.warning(f\"Refresh attempt {attempt+1}/{bucket_retry_attempts} failed for range {range_start} to {range_end} with status {status}\")\n",
    "                                if attempt == bucket_retry_attempts - 1:\n",
    "                                    # Final attempt failed - raise exception to stop processing\n",
    "                                    stats[\"failed_refreshes\"] += 1\n",
    "                                    stats[\"buckets_refreshed\"] += 1\n",
    "                                    error_msg = f\"Bucket {i+1}/{len(date_ranges)} failed after {bucket_retry_attempts} attempts with status {status}. Range: {range_start} to {range_end}\"\n",
    "                                    self.logger.error(error_msg)\n",
    "                                    raise RuntimeError(error_msg)\n",
    "                        else:\n",
    "                            self.logger.info(\"Not waiting for completion, continuing with next bucket\")\n",
    "                            bucket_success = True  # Assume success if not waiting\n",
    "                            break\n",
    "\n",
    "                    stats[\"buckets_refreshed\"] += 1\n",
    "            \n",
    "            # Case 2: Previous refresh exists and we're not reinitializing\n",
    "            else:\n",
    "                last_status = last_refresh_data.get('status')\n",
    "                last_range_start = last_refresh_data.get('range_start')\n",
    "                last_range_end = last_refresh_data.get('range_end')\n",
    "                \n",
    "                # Use stored CI/CD flag if available, otherwise use detected value\n",
    "                stored_cicd_flag = last_refresh_data.get('is_cicd_dataflow')\n",
    "                if stored_cicd_flag is not None:\n",
    "                    is_cicd_dataflow = bool(stored_cicd_flag)\n",
    "                    stats[\"is_cicd_dataflow\"] = is_cicd_dataflow\n",
    "                \n",
    "                self.logger.info(f\"Previous refresh found with status '{last_status}' for range {last_range_start} to {last_range_end}\")\n",
    "                \n",
    "                # Case 2a: Previous refresh was not successful, retry it\n",
    "                if last_status not in [\"Completed\", \"Success\", \"Successful\", \"Succeeded\"]:\n",
    "                    self.logger.info(f\"Previous refresh was not successful, retrying for range {last_range_start} to {last_range_end}\")\n",
    "\n",
    "                    # Parse dates\n",
    "                    range_start = last_range_start if isinstance(last_range_start, datetime) else self._parse_date(last_range_start)\n",
    "                    range_end = last_range_end if isinstance(last_range_end, datetime) else self._parse_date(last_range_end)\n",
    "\n",
    "                    # Retry loop for this bucket\n",
    "                    bucket_success = False\n",
    "                    for attempt in range(bucket_retry_attempts):\n",
    "                        if attempt > 0:\n",
    "                            self.logger.info(f\"Retry attempt {attempt}/{bucket_retry_attempts-1} for previously failed bucket\")\n",
    "                            stats[\"total_retry_attempts\"] += 1\n",
    "                            # Exponential backoff: wait 30, 60, 120 seconds, etc.\n",
    "                            wait_time = 30 * (2 ** (attempt - 1))\n",
    "                            self.logger.info(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                            time.sleep(wait_time)\n",
    "\n",
    "                        # Delete data in the range\n",
    "                        self.delete_data(destination_table, incremental_update_column, range_start, range_end, dataflow_id)\n",
    "\n",
    "                        # Update status to \"Running\"\n",
    "                        self.update_incremental(dataflow_id, \"Running\", range_start, range_end)\n",
    "\n",
    "                        # Close connection before long-running operation\n",
    "                        self._close_connection()\n",
    "\n",
    "                        # Trigger dataflow refresh\n",
    "                        self.refresh_dataflow(workspace_id, dataflow_id, dataflow_name, is_cicd_dataflow)\n",
    "\n",
    "                        # Wait for completion if requested\n",
    "                        if wait_for_completion:\n",
    "                            # Already closed connection above\n",
    "                            # Add a small buffer of 5 seconds before checking status\n",
    "                            time.sleep(5)\n",
    "                            status = self.wait_for_refresh_completion(workspace_id, dataflow_id, timeout_minutes, 30, is_cicd_dataflow)\n",
    "\n",
    "                            # Ensure fresh connection for database operations\n",
    "                            self._ensure_connection()\n",
    "\n",
    "                            self.update_incremental(dataflow_id, status, range_start, range_end)\n",
    "\n",
    "                            if status in [\"Success\", \"Succeeded\", \"Completed\"]:\n",
    "                                bucket_success = True\n",
    "                                stats[\"successful_refreshes\"] += 1\n",
    "                                self.logger.info(f\"Previously failed bucket completed successfully\")\n",
    "                                break  # Exit retry loop on success\n",
    "                            else:\n",
    "                                self.logger.warning(f\"Retry attempt {attempt+1}/{bucket_retry_attempts} failed for range {range_start} to {range_end} with status {status}\")\n",
    "                                if attempt == bucket_retry_attempts - 1:\n",
    "                                    # Final attempt failed - raise exception to stop processing\n",
    "                                    stats[\"failed_refreshes\"] += 1\n",
    "                                    stats[\"buckets_refreshed\"] += 1\n",
    "                                    error_msg = f\"Previously failed bucket failed after {bucket_retry_attempts} attempts with status {status}. Range: {range_start} to {range_end}\"\n",
    "                                    self.logger.error(error_msg)\n",
    "                                    raise RuntimeError(error_msg)\n",
    "                        else:\n",
    "                            bucket_success = True  # Assume success if not waiting\n",
    "                            break\n",
    "\n",
    "                    stats[\"buckets_refreshed\"] += 1\n",
    "                \n",
    "                # Case 2b: Previous refresh was successful, continue with next incremental update\n",
    "                else:\n",
    "                    self.logger.info(\"Previous refresh was successful, calculating next incremental update range\")\n",
    "                    \n",
    "                    # Parse the last range end date\n",
    "                    last_end_date = last_range_end if isinstance(last_range_end, datetime) else self._parse_date(last_range_end)\n",
    "\n",
    "                    # End date is yesterday\n",
    "                    range_end = yesterday_end\n",
    "\n",
    "                    # If incrementally_update_last_n_days is provided, use it to potentially overlap with previous data\n",
    "                    if incrementally_update_last_n_days:\n",
    "                        possible_start = yesterday - timedelta(days=incrementally_update_last_n_days)\n",
    "                        # Use the minimum of the two possible start dates to ensure there are no gaps\n",
    "                        # If last_end_date + 1 second is earlier, use that to continue from where we left off\n",
    "                        # If possible_start is earlier, use that to ensure we include the last N days\n",
    "                        range_start = min(last_end_date + timedelta(seconds=1), possible_start)\n",
    "                        self.logger.info(f\"Using start date {range_start} (minimum of last_end_date+1 second and last {incrementally_update_last_n_days} days)\")\n",
    "                    else:\n",
    "                        # Start from the day after the last end date\n",
    "                        range_start = last_end_date + timedelta(seconds=1)\n",
    "                        self.logger.info(f\"Using start date {range_start} (continuing from last end date)\")\n",
    "\n",
    "                    # Only proceed if there's actual data to refresh\n",
    "                    if range_start < range_end:\n",
    "                        self.logger.info(f\"Performing incremental update from {range_start} to {range_end}\")\n",
    "\n",
    "                        # Split the date range into buckets\n",
    "                        date_ranges = self._get_date_ranges(range_start, range_end, bucket_size_in_days)\n",
    "\n",
    "                        stats[\"date_ranges\"] = len(date_ranges)\n",
    "                        stats[\"start_date\"] = range_start\n",
    "                        stats[\"end_date\"] = range_end\n",
    "                        \n",
    "                        for i, (bucket_start, bucket_end) in enumerate(date_ranges):\n",
    "                            self.logger.info(f\"Processing bucket {i+1}/{len(date_ranges)}: {bucket_start} to {bucket_end}\")\n",
    "\n",
    "                            # Retry loop for this bucket\n",
    "                            bucket_success = False\n",
    "                            for attempt in range(bucket_retry_attempts):\n",
    "                                if attempt > 0:\n",
    "                                    self.logger.info(f\"Retry attempt {attempt}/{bucket_retry_attempts-1} for bucket {i+1}/{len(date_ranges)}\")\n",
    "                                    stats[\"total_retry_attempts\"] += 1\n",
    "                                    # Exponential backoff: wait 30, 60, 120 seconds, etc.\n",
    "                                    wait_time = 30 * (2 ** (attempt - 1))\n",
    "                                    self.logger.info(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                                    time.sleep(wait_time)\n",
    "\n",
    "                                # Delete data in the range\n",
    "                                self.delete_data(destination_table, incremental_update_column, bucket_start, bucket_end, dataflow_id)\n",
    "\n",
    "                                # Insert a record with \"Running\" status\n",
    "                                self.update_incremental(dataflow_id, \"Running\", bucket_start, bucket_end)\n",
    "\n",
    "                                # Close connection before long-running operation\n",
    "                                self._close_connection()\n",
    "\n",
    "                                # Trigger dataflow refresh\n",
    "                                self.refresh_dataflow(workspace_id, dataflow_id, dataflow_name, is_cicd_dataflow)\n",
    "\n",
    "                                # Wait for completion if requested\n",
    "                                if wait_for_completion:\n",
    "                                    # Already closed connection above\n",
    "                                    # Add a small buffer of 5 seconds before checking status\n",
    "                                    time.sleep(5)\n",
    "                                    status = self.wait_for_refresh_completion(workspace_id, dataflow_id, timeout_minutes, 30, is_cicd_dataflow)\n",
    "\n",
    "                                    # Ensure fresh connection for database operations\n",
    "                                    self._ensure_connection()\n",
    "\n",
    "                                    # Update status in the incremental table\n",
    "                                    self.update_incremental(dataflow_id, status, bucket_start, bucket_end)\n",
    "\n",
    "                                    if status in [\"Success\", \"Succeeded\", \"Completed\"]:\n",
    "                                        bucket_success = True\n",
    "                                        stats[\"successful_refreshes\"] += 1\n",
    "                                        self.logger.info(f\"Bucket {i+1}/{len(date_ranges)} completed successfully\")\n",
    "                                        break  # Exit retry loop on success\n",
    "                                    else:\n",
    "                                        self.logger.warning(f\"Refresh attempt {attempt+1}/{bucket_retry_attempts} failed for range {bucket_start} to {bucket_end} with status {status}\")\n",
    "                                        if attempt == bucket_retry_attempts - 1:\n",
    "                                            # Final attempt failed - raise exception to stop processing\n",
    "                                            stats[\"failed_refreshes\"] += 1\n",
    "                                            stats[\"buckets_refreshed\"] += 1\n",
    "                                            error_msg = f\"Bucket {i+1}/{len(date_ranges)} failed after {bucket_retry_attempts} attempts with status {status}. Range: {bucket_start} to {bucket_end}\"\n",
    "                                            self.logger.error(error_msg)\n",
    "                                            raise RuntimeError(error_msg)\n",
    "                                else:\n",
    "                                    self.logger.info(\"Not waiting for completion, continuing with next bucket\")\n",
    "                                    bucket_success = True  # Assume success if not waiting\n",
    "                                    break\n",
    "\n",
    "                            stats[\"buckets_refreshed\"] += 1\n",
    "                    else:\n",
    "                        self.logger.info(f\"No new data to refresh. Last refresh end date {last_end_date} is after or equal to start date {range_start}\")\n",
    "                        stats[\"current_status\"] = \"No new data to refresh\"\n",
    "            \n",
    "            # Calculate total duration\n",
    "            stats[\"end_time\"] = datetime.now()\n",
    "            stats[\"duration_seconds\"] = (stats[\"end_time\"] - start_time).total_seconds()\n",
    "            stats[\"current_status\"] = \"Completed\" if stats[\"failed_refreshes\"] == 0 else f\"Completed with {stats['failed_refreshes']} failures\"\n",
    "            \n",
    "            self.logger.info(f\"Incremental refresh completed for {'CI/CD ' if is_cicd_dataflow else ''}dataflow {dataflow_id}. \"\n",
    "                          f\"Total buckets: {stats['buckets_refreshed']}, \"\n",
    "                          f\"Successful: {stats['successful_refreshes']}, \"\n",
    "                          f\"Failed: {stats['failed_refreshes']}\")\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error executing incremental refresh: {str(e)}\"\n",
    "            self.logger.error(error_message)\n",
    "            \n",
    "            # Update stats with error information\n",
    "            stats[\"end_time\"] = datetime.now()\n",
    "            stats[\"duration_seconds\"] = (stats[\"end_time\"] - start_time).total_seconds()\n",
    "            stats[\"current_status\"] = f\"Failed: {str(e)}\"\n",
    "            stats[\"error\"] = str(e)\n",
    "            \n",
    "            # Ensure we close the connection on error\n",
    "            self._close_connection()\n",
    "            \n",
    "            return stats\n",
    "        finally:\n",
    "            # Make sure to close the connection when done\n",
    "            self._close_connection()\n",
    "\n",
    "# Create the power bi rest client\n",
    "client = fabric.PowerBIRestClient()\n",
    "\n",
    "# Create the dataflow refresher object\n",
    "dataflow_refresher = DataflowRefresher(\n",
    "    client, \n",
    "    CONNECTION_ARTIFACT, \n",
    "    CONNECTION_ARTIFACT_ID, \n",
    "    CONNECTION_ARTIFACT_TYPE,\n",
    "    SCHEMA, INCREMENTAL_TABLE, \n",
    "    logging.INFO)\n",
    "\n",
    "# Incrementally refresh the dataflow\n",
    "try:\n",
    "    result = dataflow_refresher.execute_incremental_refresh(\n",
    "        workspace_id,\n",
    "        dataflow_id,\n",
    "        dataflow_name,\n",
    "        destination_table,\n",
    "        incremental_update_column,\n",
    "        initial_load_from_date,\n",
    "        bucket_size_in_days,\n",
    "        reinitialize_dataflow,\n",
    "        incrementally_update_last_n_days,\n",
    "        is_cicd_dataflow=is_cicd_dataflow,\n",
    "        bucket_retry_attempts=bucket_retry_attempts\n",
    "    )\n",
    "\n",
    "    # Print the execution results\n",
    "    print(f\"Dataflow refresh execution completed:\")\n",
    "    print(f\"Status: {result.get('current_status')}\")\n",
    "    print(f\"Total buckets processed: {result.get('buckets_refreshed', 0)}\")\n",
    "    print(f\"Successful refreshes: {result.get('successful_refreshes', 0)}\")\n",
    "    print(f\"Failed refreshes: {result.get('failed_refreshes', 0)}\")\n",
    "    print(f\"Total retry attempts: {result.get('total_retry_attempts', 0)}\")\n",
    "    print(f\"Duration: {result.get('duration_seconds', 0):.2f} seconds\")\n",
    "    print(f\"Dataflow type: {'CI/CD' if result.get('is_cicd_dataflow') else 'Regular'}\")\n",
    "\n",
    "    # Exit with success code\n",
    "    import sys\n",
    "    sys.exit(0)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    # Bucket refresh failed after all retries\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATAFLOW REFRESH FAILED\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(f\"\\nThe dataflow refresh has been terminated due to bucket failure after all retry attempts.\")\n",
    "    print(f\"Please check the logs above for detailed error information.\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Exit with failure code\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "except Exception as e:\n",
    "    # Other unexpected errors\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATAFLOW REFRESH ERROR\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"An unexpected error occurred: {str(e)}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Exit with failure code\n",
    "    import sys\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7fdf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the refresh_dataflow method\n",
    "def refresh_dataflow(self, workspace_id: str, dataflow_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Refresh dataflow Gen2 and return the response\n",
    "    \n",
    "    Args:\n",
    "        workspace_id: ID of the workspace\n",
    "        dataflow_id: ID of the dataflow\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the response from the refresh API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For Dataflow Gen2 CI/CD objects, use Fabric REST API\n",
    "        response = self.client.post(f\"/v1/workspaces/{workspace_id}/items/{dataflow_id}/jobs/instances?jobType=Pipeline\", json={})\n",
    "        self.logger.info(f\"Successfully triggered refresh for dataflow Gen2 {dataflow_id}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error triggering refresh: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_latest_refresh_status(self, workspace_id: str, dataflow_id: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get refresh status of a dataflow Gen2\n",
    "    \n",
    "    Args:\n",
    "        workspace_id: ID of the workspace\n",
    "        dataflow_id: ID of the dataflow\n",
    "        \n",
    "    Returns:\n",
    "        Status of the latest refresh operation or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For Dataflow Gen2 CI/CD objects, use Fabric REST API\n",
    "        response = self.client.get(f\"/v1/workspaces/{workspace_id}/items/{dataflow_id}/jobs/instances\")\n",
    "        instances = response.json()['value']\n",
    "        if instances and len(instances) > 0:\n",
    "            latest_instance = instances[0]\n",
    "            status = latest_instance.get(\"status\", \"Unknown\")\n",
    "            self.logger.info(f\"Latest refresh status for dataflow Gen2 {dataflow_id}: {status}\")\n",
    "            return status\n",
    "        else:\n",
    "            self.logger.warning(f\"No refresh instances found for dataflow Gen2 {dataflow_id}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error checking refresh status: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "warehouse": {
    "default_warehouse": "5f958d46-a6bc-4186-87f8-e81095a3e6c1",
    "known_warehouses": [
     {
      "id": "5f958d46-a6bc-4186-87f8-e81095a3e6c1",
      "type": "Datawarehouse"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
